<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gv on gv</title>
    <link>http://geevi.github.io/</link>
    <description>Recent content in gv on gv</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Girish Varma</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0530</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Compressing Models for Recognizing Places</title>
      <link>http://geevi.github.io/publication/compression-place/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/compression-place/</guid>
      <description>&lt;p&gt;Visual place recognition on low memory devices such as mobile phones and robotics systems is a challenging problem. The state of the art models for this task uses deep learning architectures having close to 100 million parameters which takes over 400MB of memory. This makes these models infeasible to be deployed in low memory devices and gives rise to the need of compressing them. Hence we study the effectiveness of model compression techniques like trained quantization and pruning for reducing the number of parameters on one of the best performing image retrieval models called NetVLAD. We show that a compressed network can be created by starting with a model pre-trained for the task of visual place recognition and then fine-tuning it via trained pruning and quantization. The compressed model is able to produce the same mAP as the original uncompressed network. We achieve almost 50% parameter pruning with no loss in mAP and 70% pruning with close to 2% mAP reduction, while also performing 8-bit quantization. Furthermore, together with 5-bit quantization, we perform about 50% parameter reduction by pruning and get only about 3% reduction in mAP.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Super-polylogarithmic hypergraph coloring hardness via low-degree long codes</title>
      <link>http://geevi.github.io/publication/hyper-coloring-journal/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/hyper-coloring-journal/</guid>
      <description>&lt;p&gt;We prove improved inapproximability results for hypergraph coloring using the low-degree polynomial code (aka, the &amp;lsquo;short code&amp;rsquo; of Barak et. al. [FOCS 2012]) and the techniques proposed by Dinur and Guruswami [FOCS 2013] to incorporate this code for inapproximability results. In particular, we prove quasi-NP-hardness of the following problems on $n$-vertex hyper-graphs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Coloring a 2-colorable 8-uniform hypergraph with $2^{2^{\Omega(loglog\sqrt{n})}}$ colors.&lt;/li&gt;
&lt;li&gt;Coloring a 4-colorable 4-uniform hypergraph with $2^{2^{\Omega(loglog\sqrt{n})}}$ colors.&lt;/li&gt;
&lt;li&gt;Coloring a 3-colorable 3-uniform hypergraph with $(log n)^{\Omega(1/logloglog n)}$ colors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In each of these cases, the hardness results obtained are (at least) exponentially stronger than what was previously known for the respective cases. In fact, prior to this result, polylog n colors was the strongest quantitative bound on the number of colors ruled out by inapproximability results for $O(1)$-colorable hypergraphs. The fundamental bottleneck in obtaining coloring inapproximability results using the low- degree long code was a multipartite structural restriction in the PCP construction of Dinur-Guruswami. We are able to get around this restriction by simulating the multipartite structure implicitly by querying just one partition (albeit requiring 8 queries), which yields our result for 2-colorable 8-uniform hypergraphs. The result for 4-colorable 4-uniform hypergraphs is obtained via a &amp;lsquo;query doubling&amp;rsquo; method. For 3-colorable 3-uniform hypergraphs, we exploit the ternary domain to design a test with an additive (as opposed to multiplicative) noise function, and analyze its efficacy in killing high weight Fourier coefficients via the pseudorandom properties of an associated quadratic form.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hardness of Approximate Coloring</title>
      <link>http://geevi.github.io/publication/thesis/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/thesis/</guid>
      <description>&lt;p&gt;The graph coloring problem is a notoriously hard problem, for which we do not have efficient algorithms. A coloring of a graph is an assignment of colors to its vertices such that the end points of every edge have different colors. A k-coloring is a coloring that uses at most k distinct colors. The graph coloring problem is to find a coloring that uses the minimum number of colors. Given a 3-colorable graph, the best known efficient algorithms output an n0. 199···-coloring. It is known that efficient algorithms cannot find a 4-coloring, assuming &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hardness of Approximate Coloring</title>
      <link>http://geevi.github.io/project/hardness/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/hardness/</guid>
      <description>&lt;p&gt;Improved lowerbounds for graph/hypergraph coloring.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Compression</title>
      <link>http://geevi.github.io/project/model-compression/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/model-compression/</guid>
      <description>&lt;p&gt;Make deep learning models deployable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Physarum Computer</title>
      <link>http://geevi.github.io/project/physarum-computer/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/physarum-computer/</guid>
      <description>&lt;p&gt;Formal proof for slime-molds finding shortest paths in maze.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilitically Checkable Proofs</title>
      <link>http://geevi.github.io/project/pcps/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/pcps/</guid>
      <description>&lt;p&gt;Improved PCPs using low-degree codes and product constructions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streaming Algorithms</title>
      <link>http://geevi.github.io/project/streaming/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/project/streaming/</guid>
      <description>&lt;p&gt;Few pass, small memory algorithms for big data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Characterization of Hard-to-cover CSPs</title>
      <link>http://geevi.github.io/publication/characterization-covering/</link>
      <pubDate>Mon, 10 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/characterization-covering/</guid>
      <description>&lt;p&gt;We continue the study of covering complexity of constraint satisfaction problems (CSPs) initiated by Guruswami, Hastad and Sudan [SIAM J. Computing, 31(6):1663&amp;ndash;1686, 2002] and Dinur and Kol [In Proc. 28th IEEE Conference on Computational Complexity, 2013]. The covering number of a CSP instance $\phi$, denoted by ν(Φ) is the smallest number of assignments to the variables of $\phi$, such that each constraint of Φ is satisfied by at least one of the assignments. We show the following results regarding how well efficient algorithms can approximate the covering number of a given CSP instance.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assuming a covering unique games conjecture, introduced by Dinur and Kol, we show that for every non-odd predicate P over any constant sized alphabet and every integer K, it is NP-hard to distinguish between P-CSP instances (i.e., CSP instances where all the constraints are of type P) which are coverable by a constant number of assignments and those whose covering number is at least K. Previously, Dinur and Kol, using the same covering unique games conjecture, had shown a similar hardness result for every non-odd predicate over the Boolean alphabet that supports a pairwise independent distribution. Our generalization yields a complete characterization of CSPs over constant sized alphabet Σ that are hard to cover since CSP&amp;rsquo;s over odd predicates are trivially coverable with |Σ| assignments.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For a large class of predicates that are contained in the 2k-LIN predicate, we show that it is quasi-NP-hard to distinguish between instances which have covering number at most two and covering number at least Ω(loglogn). This generalizes the 4-LIN result of Dinur and Kol that states it is quasi-NP-hard to distinguish between 4-LIN-CSP instances which have covering number at most two and covering number at least Ω(logloglogn).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>On Fortification of Projection Games</title>
      <link>http://geevi.github.io/publication/fortification/</link>
      <pubDate>Mon, 10 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/fortification/</guid>
      <description>&lt;p&gt;A recent result of Moshkovitz cite{Moshkovitz14} presented an ingenious method to provide a completely elementary proof of the Parallel Repetition Theorem for certain projection games via a construction called fortification. However, the construction used in cite{Moshkovitz14} to fortify arbitrary label cover instances using an arbitrary extractor is insufficient to prove parallel repetition. In this paper, we provide a fix by using a stronger graph that we call fortifiers. Fortifiers are graphs that have both 1 and 2 guarantees on induced distributions from large subsets. We then show that an expander with sufficient spectral gap, or a bi-regular extractor with stronger parameters (the latter is also the construction used in an independent update cite{Moshkovitz15} of cite{Moshkovitz14} with an alternate argument), is a good fortifier. We also show that using a fortifier (in particular 2 guarantees) is necessary for obtaining the robustness required for fortification.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reducing uniformity in Khot-Saket hypergraph coloring hardness reductions</title>
      <link>http://geevi.github.io/publication/improved-khot-saket/</link>
      <pubDate>Sun, 10 May 2015 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/improved-khot-saket/</guid>
      <description>&lt;p&gt;In a recent result, Khot and Saket [FOCS 2014] proved the quasi-NP-hardness of coloring a 2-colorable 12-uniform hypergraph with 2(logn)Ω(1) colors. This result was proved using a novel outer PCP verifier which had a strong soundness guarantee. In this note, we show that we can reduce the arity of their result by modifying their 12-query inner verifier to an 8-query inner verifier based on the hypergraph coloring hardness reductions of Guruswami et. al. [STOC 2014]. More precisely, we prove quasi-NP-hardness of the following problems on n-vertex hypergraphs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Coloring a 2-colorable 8-uniform hypergraph with 2(logn)Ω(1) colors.&lt;/li&gt;
&lt;li&gt;Coloring a 4-colorable 4-uniform hypergraph with 2(logn)Ω(1) colors.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Derandomized Graph Product Results using the Low Degree Long Code</title>
      <link>http://geevi.github.io/publication/graph-prods/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/publication/graph-prods/</guid>
      <description>&lt;p&gt;In this paper, we address the question of whether the recent derandomization results obtained by the use of the low-degree long code can be extended to other product settings. We consider two settings: (1) the graph product results of Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the \&amp;ldquo;majority is stablest\&amp;rdquo; type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and Dinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of approximate graph coloring.&lt;/p&gt;

&lt;p&gt;In our first result, we show that there exists a considerably smaller subgraph of K⊗R3 which exhibits the following property (shown for K⊗R3 by Alon et al.): independent sets close in size to the maximum independent set are well approximated by dictators. The \&amp;ldquo;majority is stablest\&amp;rdquo; type of result of Dinur et al. and Dinur and Shinkar shows that if there exist two sets of vertices A and B in K⊗R3 with very few edges with one endpoint in A and another in B, then it must be the case that the two sets A and B share a single influential coordinate.&lt;/p&gt;

&lt;p&gt;In our second result, we show that a similar \&amp;ldquo;majority is stablest\&amp;rdquo; statement holds good for a considerably smaller subgraph of K⊗R3. Furthermore using this result, we give a more efficient reduction from Unique Games to the graph coloring problem, leading to improved hardness of approximation results for coloring.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hardness of Coloring Problems</title>
      <link>http://geevi.github.io/post/hardness-of-coloring/</link>
      <pubDate>Thu, 25 Dec 2014 00:00:00 +0530</pubDate>
      
      <guid>http://geevi.github.io/post/hardness-of-coloring/</guid>
      <description>

&lt;p&gt;This is the third part, of a series of &lt;a href=&#34;http://geevi.github.io/2014/puzzles.html&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;http://geevi.github.io/2014/approximation-limits.html&#34;&gt;posts&lt;/a&gt;,
on the impossibility of finding efficient algorithms for certain problems.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;http://geevi.github.io/2014/puzzles.html&#34;&gt;first&lt;/a&gt;, we saw that for sudoku and many other puzzles, there is a single explanation for
our inability to find polynomial time algorithms. The explanation is that, any one problem (say sudoku) that is
NP-Complete, does not have a polynomial time algorithm. This is commonly known as the P $\neq$ NP assumption.
If assuming this, a certain problem  does not have a polynomial time algorithm, then we say that it is &lt;em&gt;hard&lt;/em&gt;.
When a problem has a polynomial time algorithm, we say it is &lt;em&gt;easy&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;http://geevi.github.io/2014/approximation-limits.html&#34;&gt;second post&lt;/a&gt;, we saw that for the $3$-SAT problem, even a relaxed version of the problem, of getting an
approximation factor better than $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$, is hard. We also saw a very silly polynomial time algorithm, which gives a $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ approximation
factor. Such pair of results are called &lt;em&gt;optimal&lt;/em&gt;, since we know the exact value of the approximation factor below which the problem is easy and above which the problem is hard.&lt;/p&gt;

&lt;p&gt;The purpose of this post is to explain some
recent developments in the hardness results for a particular problem called coloring (to which I have contributed).
It is a very common problem, for which we do not have optimal results yet.&lt;/p&gt;

&lt;h4 id=&#34;graph-coloring&#34;&gt;Graph Coloring&lt;/h4&gt;

&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;../../images/highlight/graph_coloring.png&#34; width=&#34;300px&#34; /&gt; &lt;/p&gt;

&lt;p&gt;A &lt;em&gt;graph&lt;/em&gt; is an object like the uncolored figure above. It consists of a set of &lt;em&gt;vertices&lt;/em&gt;, which are the round things and a set of
&lt;em&gt;edges&lt;/em&gt;, which are the lines connecting the round things. The colored figure above is a &lt;em&gt;$3$-coloring&lt;/em&gt; of the graph on the left,
because for every edge, the vertices at the end points have different color and only $3$ colors (red, blue and green) are used.&lt;/p&gt;

&lt;p&gt;The graph coloring problem is, when given a graph (an arbitrary figure like the one on the left), find a &lt;em&gt;coloring&lt;/em&gt; (assignment of
colors to the vertices, such that the end points of every edge has different colors), using the least number of colors. In particular,
we will be looking at the following question:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Given a $3$-colorable graph, can you find a $C$-coloring?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The best polynomial time algorithm known for this problem, only guarantee $C = n^{0.199\cdots}$, where $n$ is the number of vertices. It is known
that finding a $4$-coloring  is &lt;em&gt;hard&lt;/em&gt;.  We want to know where exactly between $n^{0.199\cdots}$ and $4$, the transition from easy to hard
happens.&lt;/p&gt;

&lt;h5 id=&#34;khot-s-unique-games-conjecture-ugc&#34;&gt;Khot&amp;rsquo;s Unique Games Conjecture (UGC)&lt;/h5&gt;

&lt;p&gt;Khot observed that PCP with $2$ random queries and unique checks, will imply more  hardness of approximation results.&lt;/p&gt;

&lt;h5 id=&#34;our-results&#34;&gt;Our Results&lt;/h5&gt;

&lt;blockquote&gt;
&lt;p&gt;In &lt;a href=&#34;http://arxiv.org/abs/1411.3517&#34;&gt;joint work&lt;/a&gt; with Irit Dinur, Prahladh Harsha, Srikanth Srinivasan,
we showed that assuming a version of UGC, $C=2^{poly(\log \log n)}$ is hard, an improvement over $C=poly(\log \log n)$
hardness result of Dinur and Shinkar, using similar assumption.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Puzzle:&lt;/strong&gt; $N$ switches each of which can take $3$ positions, control a bulb which glows as red, blue and green.
Changing the position of all the switches changes the color of the bulb. Show that the bulb is controlled
by exactly one switch.&lt;/p&gt;

&lt;p&gt;Even if the number of colors the bulbs can take is $100$, for $N=1000$ this is still (al most) true.
We show that even if the switches where not allowed to move independently, such statements are still true.&lt;/p&gt;

&lt;h4 id=&#34;hypergraph-coloring&#34;&gt;Hypergraph Coloring&lt;/h4&gt;

&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;../../images/highlight/hypergraph_coloring.png&#34; /&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$r$-Uniform Hypergraph:&lt;/strong&gt; A vertex set $V$ and a
collection of subsets of $V$ of size $r$. Assign colors to
vertices such that no set is monochromatic.Graphs corresponds to the
setting of $r=2$.&lt;/p&gt;

&lt;p&gt;Known algorithms only guarantee $C=n^\alpha$ for some $\alpha &amp;lt; 1$. The state of the art results  showed that $C= poly( \log n)$ is hard.&lt;/p&gt;

&lt;h5 id=&#34;our-results-1&#34;&gt;Our Results&lt;/h5&gt;

&lt;p&gt;In &lt;a href=&#34;http://arxiv.org/abs/1311.7407&#34;&gt;joint work&lt;/a&gt; with Venkat Guruswami,
Johan Hastad, Prahladh Harsha and Srikanth Srinivasan, we show $C &amp;gt; &amp;gt; poly(\log n)$ is also hard.
Subsequent to our work, Khot and
Saket  showed $C=2^{(\log
n)^{&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;19&lt;/sub&gt;}}$ is hard. Though their result was
for $12$-uniform hypergraphs.
I further observed that
by combining their methods with ours, the same  hardness can be
obtained for $4$-uniform hypergraphs~\cite{Varma2014}.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hence  we improved the hardness results from $C=poly(\log n)$ to $C=2^{(\log n)^{&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;19&lt;/sub&gt;}}$ for the
case of $4$-colorable $4$-uniform hypergraphs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;covering-problems&#34;&gt;Covering Problems&lt;/h4&gt;

&lt;p&gt;Covering problems are a generalization of the
coloring problem. Motive for studying them is to develop
techniques which might later be adapted to graph coloring.&lt;/p&gt;

&lt;p&gt;An instance of the problem
consists of a hypergraph $G=(V,E)$ along with a &lt;em&gt;predicate&lt;/em&gt; $P
\subseteq {0,1}^r$ and a &lt;em&gt;literal&lt;/em&gt; function $L:E\rightarrow
{0,1}^r$. An &lt;em&gt;assignment&lt;/em&gt; $f:V \rightarrow {0,1}$
&lt;em&gt;covers&lt;/em&gt; an edge $e\in E$, if $f|_e \oplus L(e) \in P$.
A &lt;em&gt;cover&lt;/em&gt; for an instance is a set of
assignments such that every edge is covered by one of the assignments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The covering problem is, given a $2$-coverable instance, find a $C$-covering?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; $G$ has a cover of size $C$ with the not-all-equal predicate
${0,1}^r \setminus { \overline 0, \overline 1}$ iff it is
$2^C$-colorable.&lt;/p&gt;

&lt;p&gt;Some predicates like $3$-SAT which has an
&lt;em&gt;oddness&lt;/em&gt; property (for every $x\in {0,1}^r, x\in P \vee \bar x \in P$). Then $C=2$ is easy, since any assignment and its complement
covers the instance.&lt;/p&gt;

&lt;p&gt;Dinur and Kol asked the question
whether the covering problem is hard  for all non-odd
predicates for any constant $C$. Assuming a slightly modified form of UGC, they proceeded to show that if a non-odd predicate has a
pairwise independent distribution in its support, then this is indeed
the case.&lt;/p&gt;

&lt;h5 id=&#34;our-results-2&#34;&gt;Our Results&lt;/h5&gt;

&lt;blockquote&gt;
&lt;p&gt;In &lt;a href=&#34;http://arxiv.org/abs/1411.7747&#34;&gt;joint work&lt;/a&gt; with Amey Bhangale and Prahladh Harsha, we show&lt;br /&gt;
- hardness for all non-odd predicates and for all constants $C$ (making same assumptions as Dinur and Kol),&lt;br /&gt;
- NP-hardness under some mild conditions on the predicate for $C= \log \log n$,&lt;br /&gt;
- improved hardness of $C=\log \log n$ (Dinur and Kol proved $\log \log \log n$.).&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Approximation and its Limits </title>
      <link>http://geevi.github.io/post/approximation-limits/</link>
      <pubDate>Mon, 22 Dec 2014 00:00:00 +0530</pubDate>
      
      <guid>http://geevi.github.io/post/approximation-limits/</guid>
      <description>

&lt;p&gt;In the previous &lt;a href=&#34;http://geevi.github.io/2014/puzzles.html&#34;&gt;blog post&lt;/a&gt;, we saw that a computer as we know it, cannot help in
solving many puzzles. But some of these puzzles need to be solved, routinely in real life programs.
So should we give up hope? Is there another way out?&lt;/p&gt;

&lt;p&gt;One way out, is to relax the conditions that is required of the solutions.
For the sudoku puzzle, we might say that, we are happy with solutions with
just the rows and columns having all distinct numbers and not the blocks.
If we further remove the condition that every column has distinct elements,
then the puzzle becomes easy to solve (since for every row, we can fill
in the blanks without worrying about the other rows). For every puzzle,
we would ideally like to do minimal relaxation and come up with
a polynomial time algorithm.&lt;/p&gt;

&lt;h4 id=&#34;3-sat&#34;&gt;$3$-SAT&lt;/h4&gt;

&lt;p&gt;Now lets consider a problem which has very wide practical application. It
is called the $3$-SAT problem. An instance of the problem has a collection
of &lt;em&gt;clauses&lt;/em&gt; $C_1,\cdots,C_m$ of the following form.&lt;/p&gt;

&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;../../images/3SAT.png&#34; width=&#34;600px&#34; /&gt; &lt;/p&gt;

&lt;p&gt;Here $x_1,\cdots, x_n$ are &lt;em&gt;variables&lt;/em&gt; which takes  $0,1$ values. Here $\neg x_3$ is to be
read as NOT of $x_3$, which is the value of $1- x_3$ and $\vee$ is the OR.
An assignment to the variables &lt;em&gt;satisfies&lt;/em&gt; a clause if one among the $3$ values
associated with the OR ($\vee$) is $1$. If it satisfies all clauses then it is said
to &lt;em&gt;satisfy&lt;/em&gt; the instance and such an instance is called a &lt;em&gt;satisfiable&lt;/em&gt; instance.
 The goal is, when given a satisfiable instance,  find an
assignment of $0,1$ to the variables that satisfies the instance. This was the
first problem that was proved to be NP-Complete by Cook and Levin.&lt;/p&gt;

&lt;h4 id=&#34;approximation&#34;&gt;Approximation&lt;/h4&gt;

&lt;p&gt;A natural way to relax the problem, is to only ask for an algorithm that
finds an assignment, if it exists, that satisfies
99% of the clauses. Such an algorithm is called an &lt;em&gt;approximation
algorithm&lt;/em&gt; for the original $3$-SAT problem with &lt;em&gt;approximation factor&lt;/em&gt;   $ 0.99$.&lt;/p&gt;

&lt;p&gt;Instead of $0.99$, if we only require the &lt;em&gt;approximation factor&lt;/em&gt; to be $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$,
then there is a very stupid approximation algorithm, which does not
even need to look at instance of the problem. But this algorithm uses some
randomness and answers correctly only most of the time. Most programming
languages allow algorithms to choose random numbers during execution. Hence
these can be implemented in computers.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;All the algorithm does is for every variable, assign a random $0,1$ value. Since
a clause is the OR of $3$ values, it will not be satisfied only
when all the values are $0$, the chances of which are $&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$. Hence this
stupid algorithm gives an assignment which satisfies $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ fraction of the
clauses (most of the time).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;limits&#34;&gt;Limits&lt;/h4&gt;

&lt;p&gt;The algorithm above is very trivial. We might think that there may be an algorithm
 which, by cleverly looking at the clauses in a satisfiable problem instance, finds an assignment
 that satisfies more than a $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ fraction of the clauses.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;However in 1998, Hastad &lt;a href=&#34;http://dl.acm.org/citation.cfm?id=502098&amp;dl=ACM&amp;coll=DL&amp;CFID=468122525&amp;CFTOKEN=86867583&#34;&gt;proved&lt;/a&gt; that if there is a polynomial time algorithm with
factor better than $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$, then NP-Complete problems have polynomial time
solutions. This is not expected to happen. Hence that trivial algorithm
cannot be improved.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hastad proved similar results for many other problems. For every problem there is a magic constant
($&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ for $3$-SAT), such that there are simple polynomial time algorithms having factor equal to it. But
all known algorithms with a better approximation factor requires exponential time.&lt;/p&gt;

&lt;h4 id=&#34;probabilistically-checkable-proofs&#34;&gt;Probabilistically Checkable Proofs&lt;/h4&gt;

&lt;p&gt;Lets come back to sudoku puzzles. Some sudoku puzzle instances cannot be competed such that all
the constraints about rows, columns and sub blocks are satisfied. If it can be completed
then the instance is said to be &lt;em&gt;satisfiable&lt;/em&gt;. Then a completion that satisfies all the constraints
is a &lt;em&gt;proof&lt;/em&gt; for the fact that the instance is satisfiable. We can verify this proof easily,
but nevertheless we still need to check all the constraints about rows, columns and sub blocks
to be completely sure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A probabilistically checkable proof (PCP) is a way of writing proofs,
such that it can be checked by making a few random queries in to the proof.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PCP was discovered by Feige, Goldwasser, Lund, Safra, and Szegedy (1991), and Arora and Safra (1992).
Hastad&amp;rsquo;s result mentioned previously, showed that for any puzzle that is NP-Complete,
we can convert an instance of that puzzle to a $3$-SAT instance such that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;satisfiable instances of the puzzle are converted to satisfiable instances of $3$-SAT.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;unsatisfiable instances are converted to $3$-SAT instances for which any assignment can satisfy only a $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ fraction of the clauses.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gives a PCP for satisfiable sudoku instances as follows. First we will convert the sudoku instance
to a $3$-SAT instance and the proof will be a satisfying assignment for the $3$-SAT instance. We
will check this proof by choosing a random clause in the $3$-SAT instance, querying the $3$ variables
in it and checking if the clause is satisfied.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the sudoku instance is indeed satisfiable, then there is a proof which is the satisfying assignment for the $3$-SAT
instance that is accepted with probability $1$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Otherwise all proofs are accepted with probability at most $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$, since any assignment can satisfy only $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ fraction of clauses in the $3$-SAT instance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the above PCP, wrong proofs might get accepted with probability $&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$. But this error can be brought
down by repeatedly doing the above checks using independent random choices and rejecting the proof if any one of the
checks fails.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;One way to cop with NP-Complete puzzles, is to relax the constraints on the solutions. A
natural way to do the relaxation, for many problems like $3$-SAT, is to only look for approximation
algorithms. If the approximation factor required is small, then there are often
simple polynomial time algorithms. When the factor required is larger than a threshold ($&lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;$ for $3$-SAT),
then polynomial time algorithms could not be found. An explanation for our inability for doing
this could be obtained from the conjecture that NP-Complete problems does not have polynomial time algorithms.
Research on this has lead to the discovery of fascinating objects like  probabilistically checkable proofs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Puzzling Computers</title>
      <link>http://geevi.github.io/post/puzzling-computers/</link>
      <pubDate>Mon, 22 Dec 2014 00:00:00 +0530</pubDate>
      
      <guid>http://geevi.github.io/post/puzzling-computers/</guid>
      <description>

&lt;h4 id=&#34;sudoku&#34;&gt;Sudoku&lt;/h4&gt;

&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;../../images/sudoku_9x9.png&#34; width=&#34;200px&#34; /&gt; &lt;img src=&#34;../../images/sudoku_9x9_solved.png&#34; width=&#34;200px&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Above is an &lt;em&gt;instance&lt;/em&gt; of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Sudoku&#34; target=&#34;_blank&#34;&gt;sudoku&lt;/a&gt;
puzzle. The goal is to fill in numbers from $1$ to $9$ in the blanks such that,
every  row, column and $3 \times 3$ block has all the $9$ different numbers.
Beside is a solution for the puzzle instance and it is easy to verify that the
&lt;em&gt;constraints&lt;/em&gt; on the rows, columns and sub blocks are &lt;em&gt;satisfied&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Since very few numbers are given, it is difficult to solve.
Nevertheless a person might solve it in a few
hours. But what if the puzzle instance was &amp;ldquo;larger&amp;rdquo;.
That is instead of a $9\times 9$, it is a $16\times 16$ version,
where numbers from $1$ to $16$ could be filled in.
It has similar &lt;em&gt;constraints&lt;/em&gt; on having all
numbers in rows and columns and $4\times 4$ sub blocks.&lt;/p&gt;

&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;../../images/sudoku_16x16.png&#34; width=&#34;400px&#34; style=&#34;margin: 10px 20px&#34;/&gt; &lt;/p&gt;

&lt;p&gt;Such a puzzle would be mind boggling for
humans to handle. But, may be there is a
clever computer program, to do the job for
us. There is some hope, since a solution
given below for the above sudoku instance is very
easy to verify.&lt;/p&gt;

&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;../../images/sudoku_16x16_solved.png&#34; width=&#34;400px&#34; style=&#34;margin: 10px 20px&#34;/&gt; &lt;/p&gt;

&lt;p&gt;One just needs to check if every row, column and
sub block has all the numbers from $1$ to $16$. So
 an &lt;em&gt;algorithm&lt;/em&gt; (a computer program) would be to
go over all &lt;em&gt;assignments&lt;/em&gt; of numbers from $1$ to $16$, to the blanks
and check if the &lt;em&gt;constraints&lt;/em&gt; on rows, columns and
sub blocks are &lt;em&gt;satisfied&lt;/em&gt;. This might look
like a simple thing to do, except when one tries
to run the program.&lt;/p&gt;

&lt;h4 id=&#34;polynomial-time-algorithms&#34;&gt;Polynomial Time Algorithms&lt;/h4&gt;

&lt;p&gt;Suppose half the blanks in the above $16\times 16$
 puzzle instance were
already filled, the number of possibilities
the program has to try is $16^{16 \times &lt;sup&gt;16&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}$ (try all the $16$ possibilities
for each of the $16\times &lt;sup&gt;16&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;$ blanks). This number is
more than the &lt;a href=&#34;http://en.wikipedia.org/wiki/Chronology_of_the_universe&#34; target=&#34;_blank&#34;&gt;number&lt;/a&gt;
 of seconds since the Big Bang and
more than the &lt;a href=&#34;http://www.physicsoftheuniverse.com/numbers.html&#34; target=&#34;_blank&#34;&gt;number&lt;/a&gt;
 of particles in the universe. No
matter how fast or large a computer we have, it will
keep running (if we can maintain it that way) &lt;a href=&#34;http://image.gsfc.nasa.gov/poetry/ask/a10395.html&#34; target=&#34;_blank&#34;&gt;till&lt;/a&gt;
our sun goes dead.&lt;/p&gt;

&lt;p&gt;Hence the &amp;ldquo;try all possibilities&amp;rdquo; algorithm is definitely not
acceptable. We want an algorithm which, lets say
finds the solution in a day. As we saw, we would
also like to solve &amp;ldquo;larger&amp;rdquo; $n\times n$ versions of the puzzle.
$9\times 9$ was the case for $n=9$ and $16\times 16$ for $n=16$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The try
all possibilities algorithm that we saw, needs to try (at least)
$n^n$ possibilities. Algorithms with such a bad running time are called
&lt;em&gt;exponential time&lt;/em&gt; algorithms. Even for $n=16$, $n^n$ is a humongous number,
because n is in the &lt;em&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Exponentiation&#34; target=&#34;_blank&#34;&gt;exponent&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Suppose the number of steps
the algorithm runs is $n^c$ for some constant $c$, it would not
be that bad. Such an algorithm is called a  &lt;em&gt;polynomial time&lt;/em&gt;  algorithm.
Finding a polynomial time algorithm for sudoku is a long standing
challenge for programmers, which remains unsolved till this date.
All the algorithms known so far can
take $n^n$ time on some sudoku instances.&lt;/p&gt;

&lt;h4 id=&#34;np-completeness&#34;&gt;NP-Completeness&lt;/h4&gt;

&lt;p&gt;In the 1970&amp;rsquo;s, Stephen Cook, Leonid Levin and Richard Karp found that sudoku is not just a one off
case, for which polynomial time algorithms cannot be found. They showed
that the same mystery exists for a whole class of puzzles, called NP-Complete (NPC).&lt;/p&gt;

&lt;p&gt;They showed this, by devising a way of solving any one puzzle in NPC, by
using solutions to any other puzzle in NPC. That is for any pair of puzzles,
say sudoku and &lt;a href=&#34;http://en.wikipedia.org/wiki/Kakuro&#34; target=&#34;_blank&#34;&gt;kakuro&lt;/a&gt; in NPC,
 they showed how to convert an instance of
sudoku to an instance of kakuro such that a solution to the kakuro instance
can be converted back to a solution to the sudoku instance. If that was confusing,
what it means is that,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;if there is a polynomial time algorithm for any one puzzle
in NPC, that would mean there is a polynomial time algorithm for all puzzles in NPC.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Till today, thousands of problems has been found to be NP-Complete and we dont have algorithms
that runs in time much better than $n^n$ for all of them. By the result mentioned above, if there
was a better algorithm for any one of them, then there is an algorithm with very similar running time
for each NP-Complete problem.&lt;/p&gt;

&lt;h4 id=&#34;why-care&#34;&gt;Why Care?&lt;/h4&gt;

&lt;p&gt;Who cares about solving puzzles when there are
more pressing problems in the world. These might appear to be
questions that a jobless unrealistic philosopher might ponder
about. Though it is not so. A lot depends on finding answers
to these questions.&lt;/p&gt;

&lt;p&gt;Nobody would disagree that internet is one of the greatest things
 to happen in end of the previous century.  An
underlying technology that makes it possible is encryption.
That is, a way of sending messages so that only the intended person
is able to read it.&lt;/p&gt;

&lt;p&gt;All the existing methods of encryption are based on the assumption
that NPC problems does not have polynomial time algorithms. That is&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;if somebody comes up with a fast polynomial time algorithm for solving
sudoku puzzles, he can break all the codes in the internet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dont panic!. Your internet banking accounts are safe. We
dont expect this to happen. However, we need to know what other
problems are in NPC. This will help us design better encryption
systems and many other &lt;a href=&#34;http://en.wikipedia.org/wiki/Zero-knowledge_proof&#34; target=&#34;_blank&#34;&gt;things&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;a-philosophical-detour&#34;&gt;A Philosophical Detour&lt;/h4&gt;

&lt;p&gt;Though most people are interested in solving those pressing problems
of the world, there are still some stupid harmless philosophers
who find it interesting to think about these for the sake of curiosity.
They find a &amp;ldquo;deeper&amp;rdquo; meaning to this silly question about solving
puzzles.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Mathematical proofs, sudoku solutions are all easy to verify. Does that
mean they are easy to come up with? Can human creativity be truly
automated?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If NPC puzzles had fast polynomial time algorithms, then in many cases
humans can be replaced by computers in solving problems. The concepts of
puzzles and NPC described in this article could be
defined unambiguously in the language of maths. Scientists have been
hard at work for over 40 years either trying show NPC does not have polynomial time
algorithms or finding fast algorithms for these problems. Some day, hopefully
soon, we will have an answer.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;The results mentioned above increases our faith in the conjecture that NP-Complete problems does not
have polynomial time algorithms.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This conjecture could be disproved if any one of the thousands of NP-Complete problems has a polynomial time algorithm, but yet it has stood the test of time for over 40 years. Assuming this conjecture gives a singular explanation for our inability to design good algorithms
for many problems.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;further-reading&#34;&gt;Further Reading&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://geevi.github.io/2014/approximation-limits.html&#34;&gt;What to do, when a puzzle is NP-Complete?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
