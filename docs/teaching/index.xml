<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Teachings on Girish varma</title>
    <link>http://geevi.github.io/teaching/</link>
    <description>Recent content in Teachings on Girish varma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Girish Varma</copyright>
    <lastBuildDate>Mon, 01 Jan 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/teaching/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Efficient CNNs</title>
      <link>http://geevi.github.io/teaching/efficient-cnns/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/teaching/efficient-cnns/</guid>
      <description>

&lt;p&gt;Deep Neural Networks, while being unreasonably effective for several vision tasks, have their usage limited by the computational and memory requirements, both during training and inference stages. Analyzing and improving the connectivity patterns between layers of a network has resulted in several compact architectures like GoogleNet, ResNet and DenseNet-BC. We survey the most recent developments giving CNN architectures with better error vs resourse (parameters/FLOPs/energy/memory/fps) tradeoffs.&lt;/p&gt;

&lt;p&gt;We also plan to learn how to do fast implementations of CNNs in existing processing architectures. Most important processor currently is the GPU. Hence we plan to learn some amount of GPU programming as well.&lt;/p&gt;

&lt;h2 id=&#34;schedule&#34;&gt;Schedule&lt;/h2&gt;

&lt;p&gt;Every week we meet for 3 hrs. Broadly it will be divided in to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1 hr will be spent on basics and pre 2017 techniques&lt;/li&gt;
&lt;li&gt;1 hr on more recent techniques or theory&lt;/li&gt;
&lt;li&gt;1 hr on GPU programming (also some thing about FPGAs or general hardware?)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;

&lt;p&gt;The topics covered can be broadly classified into following. It is reccomended that each participant take one topic and read all papers related to it. If there are too many in a particular topic, we can find some way to share the load.&lt;/p&gt;

&lt;h3 id=&#34;explicit-compression-techniques&#34;&gt;Explicit Compression Techniques&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Trained Pruning and Quantization : &lt;a href=&#34;https://arxiv.org/abs/1510.00149&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1510.00149&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Low Rank and Sparse Approximations : &lt;a href=&#34;http://ieeexplore.ieee.org/document/8099498/&#34; target=&#34;_blank&#34;&gt;http://ieeexplore.ieee.org/document/8099498/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binarized Networks : &lt;a href=&#34;http://allenai.org/plato/xnornet/&#34; target=&#34;_blank&#34;&gt;http://allenai.org/plato/xnornet/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;efficient-cnn-designs&#34;&gt;Efficient CNN Designs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Grouped Convolutions :

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ikhlestov.github.io/pages/machine-learning/convolutions-types/&#34; target=&#34;_blank&#34;&gt;https://ikhlestov.github.io/pages/machine-learning/convolutions-types/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.yani.io/filter-group-tutorial/&#34; target=&#34;_blank&#34;&gt;https://blog.yani.io/filter-group-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Depthwise Seperable Convolutions (MobileNet)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1704.04861&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ResNext

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.05431&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1611.05431&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DenseNet

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1608.06993&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1608.06993&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ShuffleNet

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.01083&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1707.01083&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Interleaved Convolution

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.02725.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1707.02725.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Expander Nets

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.08757&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1711.08757&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ZeroFLOP operation

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.08141&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1711.08141&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Block Sparse GPU Kernels

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.openai.com/block-sparse-gpu-kernels/&#34; target=&#34;_blank&#34;&gt;https://blog.openai.com/block-sparse-gpu-kernels/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Capsule Network

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.09829&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1710.09829&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;FractalNet

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1605.07648&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1605.07648&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;MEC (ICML &amp;lsquo;17)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v70/cho17a/cho17a.pdf&#34; target=&#34;_blank&#34;&gt;http://proceedings.mlr.press/v70/cho17a/cho17a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;SENet (ImageNet 2017 Competition Winner)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.01507.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1709.01507.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;MobileNet v2 (with detection and segmentation scores)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1801.04381.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1801.04381.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;efficient-semantic-segmentation-architectures&#34;&gt;Efficient Semantic Segmentation Architectures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;DeepLab V3, PSPNet (Atrous Convolutions, Pyramidal Spatial Pooling)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.05587&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1706.05587&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hszhao.github.io/projects/pspnet/&#34; target=&#34;_blank&#34;&gt;https://hszhao.github.io/projects/pspnet/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;architecture-search&#34;&gt;Architecture Search&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;NasNet

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://research.googleblog.com/2017/11/automl-for-large-scale-image.html&#34; target=&#34;_blank&#34;&gt;https://research.googleblog.com/2017/11/automl-for-large-scale-image.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.07012&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1707.07012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Hierarchical Representations for Efficient Architecture Search

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.00436&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1711.00436&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Progressive Neural Architecture Search

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.00559&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1712.00559&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CondenseNet

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.09224&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1711.09224&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;gpu-programming&#34;&gt;GPU Programming&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Follow course notes : &lt;a href=&#34;http://courses.cms.caltech.edu/cs179/&#34; target=&#34;_blank&#34;&gt;http://courses.cms.caltech.edu/cs179/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;theory-for-cnns&#34;&gt;Theory for CNNs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Follow some papers from : &lt;a href=&#34;https://stats385.github.io/readings&#34; target=&#34;_blank&#34;&gt;https://stats385.github.io/readings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;meetings&#34;&gt;Meetings&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;CNN Introduction and Survey | Depthwise Seperarable Convolutions (Inception, Xception, MobileNet) | Pruning and Quantization Introduction&lt;/li&gt;
&lt;li&gt;ResNext | GPU Programming 1&lt;/li&gt;
&lt;li&gt;FractualNets | Memory Efficient Convolutions (MEC)&lt;/li&gt;
&lt;li&gt;CapsuleNet | tvmlang | Semantic segmentation architectures (PSP Module, Atrous convolutions from Deeplab v3)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;participants&#34;&gt;Participants&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Ameya Prabhu&lt;/li&gt;
&lt;li&gt;Aniruddha Vivek Patil&lt;/li&gt;
&lt;li&gt;Sriharsha Annamaneni&lt;/li&gt;
&lt;li&gt;Aaron Varghese&lt;/li&gt;
&lt;li&gt;Vallurupalli Nikitha&lt;/li&gt;
&lt;li&gt;Sudhir Kumar Reddy&lt;/li&gt;
&lt;li&gt;Soham Saha&lt;/li&gt;
&lt;li&gt;Ashutosh Mishra&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multi Object Tracking using Deep Learning</title>
      <link>http://geevi.github.io/teaching/mot/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/teaching/mot/</guid>
      <description>

&lt;p&gt;Multi Object Tracking is the task tracking objects in video frames. We survey the various types of MOT methods, with special focus on the latest methods that use Deep Learning.&lt;/p&gt;

&lt;h3 id=&#34;papers&#34;&gt;Papers&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Learning to Track: Online Multi-object Tracking by Decision Making&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ICCV&amp;rsquo;15&lt;/li&gt;
&lt;li&gt;Yu Xiang, Alexandre Alahi, Silvio Savarese&lt;/li&gt;
&lt;li&gt;cvgl.stanford.edu/papers/xiang_iccv15.pdf&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Online Multi-Target Tracking Using Recurrent Neural Networks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AAAI&amp;rsquo;17&lt;/li&gt;
&lt;li&gt;Anton Milan, Seyed Hamid Rezatofighi, Anthony Dick, Ian Reid, Konrad Schindler&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1604.03635&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1604.03635&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep Network Flow for Multi-Object Tracking&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CVPR &amp;lsquo;17&lt;/li&gt;
&lt;li&gt;Samuel Schulter, Paul Vernaza, Wongun Choi, Manmohan Chandraker&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.08482&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1706.08482&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learning to Track at 100 FPS with Deep Regression Networks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ECCV &amp;lsquo;16&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-46448-0_45&#34; target=&#34;_blank&#34;&gt;https://link.springer.com/chapter/10.1007/978-3-319-46448-0_45&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep tracking in the wild: End-to-end tracking using recurrent neural networks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IJRR &amp;lsquo;17&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~mobile/Papers/2017_IJRR_Dequaire.pdf&#34; target=&#34;_blank&#34;&gt;http://www.robots.ox.ac.uk/~mobile/Papers/2017_IJRR_Dequaire.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Julie Dequaire, Peter Ondrúška, Dushyant Rao, Dominic Wang, and Ingmar Posner&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Survey Slides by Merc Benz&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://32e624bb-a-62cb3a1a-s-sites.googlegroups.com/site/dlitsc17/A%20Survey%20on%20Leveraging%20Deep%20Neural%20Networks%20for.pdf&#34; target=&#34;_blank&#34;&gt;https://32e624bb-a-62cb3a1a-s-sites.googlegroups.com/site/dlitsc17/A%20Survey%20on%20Leveraging%20Deep%20Neural%20Networks%20for.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multiple Object Tracking: A Literature Review&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1409.7618&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1409.7618&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A survey on multiple object tracking algorithm&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/7832121/&#34; target=&#34;_blank&#34;&gt;http://ieeexplore.ieee.org/document/7832121/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multiple Object Tracking: A Literature Review&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/8b42/8625c8a496c7e56b419197df55c751c22bc3.pdf&#34; target=&#34;_blank&#34;&gt;https://pdfs.semanticscholar.org/8b42/8625c8a496c7e56b419197df55c751c22bc3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;students&#34;&gt;Students&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Vidit Jain&lt;/li&gt;
&lt;li&gt;Mohammed Sharfuddin&lt;/li&gt;
&lt;li&gt;Anjan Kumar&lt;/li&gt;
&lt;li&gt;Haard Panjal&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Complexity Theory</title>
      <link>http://geevi.github.io/teaching/complexity-theory/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/teaching/complexity-theory/</guid>
      <description>

&lt;p&gt;Complexity theory studies what problems can and cannot be solved by computers given limited running time and memory resources.&lt;/p&gt;

&lt;p&gt;Students are expected to read the corresponding chapter from the textbook (mentioned under readings), before attending the lecture.
The lecture is mainly for clearing doubts and covering the most confusing parts.&lt;/p&gt;

&lt;p&gt;The is a short course with 13 lectures for students who might not have
taken a Theory of Computation course. However sufficient knowledge of
discrete maths and algorithms is assumed. It is not supposed to be a
rigorous one, for research student, but rather an introduction to field
of complexity theory.&lt;/p&gt;

&lt;h3 id=&#34;timings&#34;&gt;Timings&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Wednesday (10:00 a.m.-11:25 a.m., Venue: 205)&lt;/li&gt;
&lt;li&gt;Saturday (10:00 a.m.-11:25 a.m., Venue: 205)&lt;/li&gt;
&lt;li&gt;Tutorial : Saturday (3:30 p.m.-4:30 p.m., Venue: 204)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;lectures&#34;&gt;Lectures&lt;/h3&gt;

&lt;p&gt;Short notes are available with additional references : &lt;a href=&#34;notes&#34; target=&#34;_blank&#34;&gt;Consolidated Notes HTML&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt; &lt;br /&gt;
Decision Problems | Turing Machines | Efficient Algorithms&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Paradox&amp;rsquo;s, Diagonalization, Undecidablility&lt;/strong&gt; &lt;br /&gt;
Russell&amp;rsquo;s Paradox and Diagonalization | Universal Turing Machines | Halting Problem&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Nondeterminism, NP and Search Problems&lt;/strong&gt; &lt;br /&gt;
Non Deterministic Turing Machines | Nondeterministic Polynomial Time : NP | NP : Verifier Definition | Descision vs Search Problems&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reductions, NP-Completenesss &amp;amp; Cooks Theorem&lt;/strong&gt; &lt;br /&gt;
Polynomial Time Reductions | Cook-Levin Theorem | NP Compeleness / Hardness&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;More Time Complexity&lt;/strong&gt; &lt;br /&gt;
NP-completeness of Vertex Cover and Subset Sum | EXP and Time Heirachy Theorem | coNP and Map of Complexity classes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Space Compleixty&lt;/strong&gt; &lt;br /&gt;
Space Complexity classes and some Relationships | Non Determinism and Space : NL-complete, Savith&amp;rsquo;s Theorem | Overview of next part of course&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;NL = coNL, Randomization&lt;/strong&gt; &lt;br /&gt;
Recapping Nondeterminism | Immerman-Szelepcsenyi Theorem : NL = coNL | Randomized TMs and Complexity Classes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Randomized Algorithms&lt;/strong&gt; &lt;br /&gt;
BPP and Amplification | Approx MAX-CUT | Undirected REACHABILITY in RL&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Random Walks, Derandomization and Polynomial Identity Testing&lt;/strong&gt; &lt;br /&gt;
Analysis of Random Walks | Derandomizing MaxCUT | Randomization: PIT and Schwartz-Zippel Lemma&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Streaming Algorithms and Lowerbounds&lt;/strong&gt;  &lt;br /&gt;
Schwartz Zippel Lemma Proof | Lowerbound for Bracket Matching | Randomized Streaming Algorithm for Bracket Matching&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Property Testing&lt;/strong&gt; &lt;br /&gt;
Property Testing Model | Linearity Test by Blum, Luby and Rubinfield&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;PAC Learning&lt;/strong&gt;&lt;br /&gt;
hugo&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;grading&#34;&gt;Grading&lt;/h3&gt;

&lt;p&gt;This part of the course will have a total of 50 marks + 5 bonus marks.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;25 marks for the end sem.&lt;/li&gt;
&lt;li&gt;15 marks for the mid sem.&lt;/li&gt;
&lt;li&gt;10 marks for assignments.&lt;/li&gt;
&lt;li&gt;5 marks for quiz.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2-3 question will be given out at the end of every lecture. There will be one tutorial
session per week (to be scheduled), where the assignments for the previous week will be collected, and
solutions will be discussed.&lt;/p&gt;

&lt;h3 id=&#34;textbook-and-references&#34;&gt;Textbook and References&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;notes&#34; target=&#34;_blank&#34;&gt;Notes&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Textbook : Computational Complexity by Christos Papadimitriou&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some courses offered elsewhere:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Sud] A course on complexity theory (offered by Madhu Sudan) at MIT: 6.&lt;sup&gt;841&lt;/sup&gt;&amp;frasl;&lt;sub&gt;18&lt;/sub&gt;.405J Advanced Complexity Theory (Spring 2003).&lt;/li&gt;
&lt;li&gt;[Tre1] A course on complexity theory (offered by Luca Trevisan) at UC, Berkeley: CS 278 - Computational Complexity (Fall 2002).&lt;/li&gt;
&lt;li&gt;[Tre2] A course on complexity theory (offered by Luca Trevisan) at UC, Berkeley: CS 278 - Computational Complexity (Fall 2004).&lt;/li&gt;
&lt;li&gt;[Vad] A course of complexity theory (offered by Salil Vadhan) at Harvard: CS221: Computational Complexity (Spring 2010).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://geevi.github.io/teaching/complexity-theory/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://geevi.github.io/teaching/complexity-theory/notes/</guid>
      <description>Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
--&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&#34;Content-Type&#34; content=&#34;text/html; charset=utf-8&#34; /&gt;
&lt;meta name=&#34;generator&#34; content=&#34;DocOnce: https://github.com/hplgit/doconce/&#34; /&gt;
&lt;meta name=&#34;description&#34; content=&#34;A Short Course on Complexity Theory&#34;&gt;

&lt;title&gt;A Short Course on Complexity Theory&lt;/title&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;style.css&#34;&gt;




&lt;/head&gt;

&lt;!-- tocinfo
{&#39;highest level&#39;: 0,
 &#39;sections&#39;: [(&#39;Table of contents&#39;,
               0,
               &#39;table_of_contents&#39;,
               &#39;table_of_contents&#39;),
              (&#39;Lecture 1: Introduction&#39;, 0, None, &#39;___sec0&#39;),
              (&#39;Decision Problems&#39;, 1, None, &#39;___sec1&#39;),
              (&#39;Turing Machines&#39;, 1, None, &#39;___sec2&#39;),
              (&#39;Robustness of TM&#39;, 2, None, &#39;___sec3&#39;),
              (&#39;Efficient Algorithms&#39;, 1, None, &#39;___sec4&#39;),
              (&#39;Polynomial time vs Exponential Time&#39;, 2, None, &#39;___sec5&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec6&#39;),
              (&#34;Lecture 2: Paradox&#39;s and Diagonalization&#34;,
               0,
               None,
               &#39;___sec7&#39;),
              (&#34;Russell&#39;s Paradox and Diagonalization&#34;, 1, None, &#39;___sec8&#39;),
              (&#39;Universal Turing Machines&#39;, 1, None, &#39;___sec9&#39;),
              (&#39;Halting Problem&#39;, 1, None, &#39;___sec10&#39;),
              (&#39;Lecture 3: Non Determinism, NP and Search Problems&#39;,
               0,
               None,
               &#39;___sec11&#39;),
              (&#39;Non Deterministic Turing Machines&#39;, 1, None, &#39;___sec12&#39;),
              (&#39;Nondeterministic Polynomial Time : NP&#39;, 1, None, &#39;___sec13&#39;),
              (&#39;NP : Verifier Definition&#39;, 1, None, &#39;___sec14&#39;),
              (&#39;Descision vs Search Problems&#39;, 1, None, &#39;___sec15&#39;),
              (&#39;Lecture 4: Reductions, Cook-Levin Theorem and NP-Completeness&#39;,
               0,
               None,
               &#39;___sec16&#39;),
              (&#39;Polynomial Time Reductions&#39;, 1, None, &#39;___sec17&#39;),
              (&#39;Cook-Levin Theorem&#39;, 1, None, &#39;___sec18&#39;),
              (&#39;NP Compeleness / Hardness&#39;, 1, None, &#39;___sec19&#39;),
              (&#39;Lecture 5: More Time Complexity&#39;, 0, None, &#39;___sec20&#39;),
              (&#39;NP-completeness of Vertex Cover and Subset Sum&#39;,
               1,
               None,
               &#39;___sec21&#39;),
              (&#39;EXP and Time Heirachy Theorem&#39;, 1, None, &#39;___sec22&#39;),
              (&#39;coNP and Map of Complexity classes&#39;, 1, None, &#39;___sec23&#39;),
              (&#39;Lecture 6: Space Complexity&#39;, 0, None, &#39;___sec24&#39;),
              (&#39;Space Complexity classes and some Relationships&#39;,
               1,
               None,
               &#39;___sec25&#39;),
              (&#34;Non Determinism and Space : NL-complete, Savith&#39;s Theorem&#34;,
               1,
               None,
               &#39;___sec26&#39;),
              (&#39;Overview of next part of course&#39;, 1, None, &#39;___sec27&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec28&#39;),
              (&#39;Lecture 7: NL = coNL, Randomization&#39;, 0, None, &#39;___sec29&#39;),
              (&#39;Recapping Nondeterminism&#39;, 1, None, &#39;___sec30&#39;),
              (&#39;Immerman-Szelepcsenyi Theorem : NL = coNL&#39;,
               1,
               None,
               &#39;___sec31&#39;),
              (&#39;Randomized TMs and Complexity Classes&#39;, 1, None, &#39;___sec32&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec33&#39;),
              (&#39;Lecture 8: Randomized Algos&#39;, 0, None, &#39;___sec34&#39;),
              (&#39;BPP and Amplification&#39;, 1, None, &#39;___sec35&#39;),
              (&#39;Approx MAX-CUT&#39;, 1, None, &#39;___sec36&#39;),
              (&#39;Undirected REACHABILITY in RL&#39;, 1, None, &#39;___sec37&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec38&#39;),
              (&#39;Lecture 9: Random Walks, Derandomization and PIT&#39;,
               0,
               None,
               &#39;___sec39&#39;),
              (&#39;Analysis of Random Walks&#39;, 1, None, &#39;___sec40&#39;),
              (&#39;Derandomizing MaxCUT&#39;, 1, None, &#39;___sec41&#39;),
              (&#39;Randomization: PIT and Schwartz-Zippel Lemma&#39;,
               1,
               None,
               &#39;___sec42&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec43&#39;),
              (&#39;Lecture 10: Streaming Algorithms and Lowerbounds&#39;,
               0,
               None,
               &#39;___sec44&#39;),
              (&#39;Schwartz Zippel Lemma Proof&#39;, 1, None, &#39;___sec45&#39;),
              (&#39;Lowerbound for Bracket Matching&#39;, 1, None, &#39;___sec46&#39;),
              (&#39;Randomized Streaming Algorithm for Bracket Matching&#39;,
               1,
               None,
               &#39;___sec47&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec48&#39;),
              (&#39;Lecture 11: Property Testing&#39;, 0, None, &#39;___sec49&#39;),
              (&#39;Property Testing Model&#39;, 1, None, &#39;___sec50&#39;),
              (&#39;Linearity Test by Blum, Luby and Rubinfield&#39;,
               1,
               None,
               &#39;___sec51&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec52&#39;),
              (&#39;Lecture 12: Learning Theory&#39;, 0, None, &#39;___sec53&#39;),
              (&#39;Probably Approximately Correct!&#39;, 1, None, &#39;___sec54&#39;),
              (&#39;Learning Rectangles&#39;, 1, None, &#39;___sec55&#39;),
              (&#39;Learning CNFs&#39;, 1, None, &#39;___sec56&#39;),
              (&#39;Readings&#39;, 1, None, &#39;___sec57&#39;)]}
end of tocinfo --&gt;

&lt;body&gt;



&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: &#34;AMS&#34;  },
     extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;, &#34;autobold.js&#34;, &#34;color.js&#34;]
  }
});
&lt;/script&gt;
&lt;script type=&#34;text/javascript&#34; async
 src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;



    
&lt;!-- ------------------- main content ---------------------- --&gt;



&lt;center&gt;&lt;h1&gt;A Short Course on Complexity Theory&lt;/h1&gt;&lt;/center&gt;  &lt;!-- document title --&gt;

&lt;p&gt;
&lt;!-- author(s): Girish Varma EMAIL: girish.varma@iiit.ac.in --&gt;

&lt;center&gt;
&lt;b&gt;Girish Varma EMAIL: girish.varma@iiit.ac.in&lt;/b&gt; 
&lt;/center&gt;

&lt;p&gt;
&lt;!-- institution --&gt;

&lt;center&gt;&lt;b&gt;Machine Learning Lab, IIIT Hyderabad&lt;/b&gt;&lt;/center&gt;
&lt;br&gt;
&lt;p&gt;
&lt;center&gt;&lt;h4&gt;Nov 11, 2017&lt;/h4&gt;&lt;/center&gt; &lt;!-- date --&gt;
&lt;br&gt;

&lt;h1 id=&#34;table_of_contents&#34;&gt;Table of contents&lt;/h2&gt;

&lt;p&gt;
&lt;a href=&#34;#___sec0&#34;&gt; Lecture 1: Introduction &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec1&#34;&gt; Decision Problems &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec2&#34;&gt; Turing Machines &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec3&#34;&gt; Robustness of TM &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec4&#34;&gt; Efficient Algorithms &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec5&#34;&gt; Polynomial time vs Exponential Time &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec6&#34;&gt; Readings &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec7&#34;&gt; Lecture 2: Paradox&#39;s and Diagonalization &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec8&#34;&gt; Russell&#39;s Paradox and Diagonalization &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec9&#34;&gt; Universal Turing Machines &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec10&#34;&gt; Halting Problem &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec11&#34;&gt; Lecture 3: Non Determinism, NP and Search Problems &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec12&#34;&gt; Non Deterministic Turing Machines &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec13&#34;&gt; Nondeterministic Polynomial Time : NP &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec14&#34;&gt; NP : Verifier Definition &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec15&#34;&gt; Descision vs Search Problems &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec16&#34;&gt; Lecture 4: Reductions, Cook-Levin Theorem and NP-Completeness &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec17&#34;&gt; Polynomial Time Reductions &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec18&#34;&gt; Cook-Levin Theorem &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec19&#34;&gt; NP Compeleness / Hardness &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec20&#34;&gt; Lecture 5: More Time Complexity &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec21&#34;&gt; NP-completeness of Vertex Cover and Subset Sum &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec22&#34;&gt; EXP and Time Heirachy Theorem &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec23&#34;&gt; coNP and Map of Complexity classes  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec24&#34;&gt; Lecture 6: Space Complexity &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec25&#34;&gt; Space Complexity classes and some Relationships &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec26&#34;&gt; Non Determinism and Space : NL-complete, Savith&#39;s Theorem &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec27&#34;&gt; Overview of next part of course  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec28&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec29&#34;&gt; Lecture 7: NL = coNL, Randomization &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec30&#34;&gt; Recapping Nondeterminism &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec31&#34;&gt; Immerman-Szelepcsenyi Theorem : NL = coNL &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec32&#34;&gt; Randomized TMs and Complexity Classes &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec33&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec34&#34;&gt; Lecture 8: Randomized Algos &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec35&#34;&gt;  BPP and Amplification  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec36&#34;&gt;  Approx MAX-CUT  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec37&#34;&gt; Undirected REACHABILITY in RL &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec38&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec39&#34;&gt; Lecture 9: Random Walks, Derandomization and PIT  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec40&#34;&gt; Analysis of Random Walks &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec41&#34;&gt; Derandomizing MaxCUT &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec42&#34;&gt; Randomization: PIT and Schwartz-Zippel Lemma &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec43&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec44&#34;&gt; Lecture 10: Streaming Algorithms and Lowerbounds  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec45&#34;&gt; Schwartz Zippel Lemma Proof  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec46&#34;&gt; Lowerbound for Bracket Matching  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec47&#34;&gt; Randomized Streaming Algorithm for Bracket Matching  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec48&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec49&#34;&gt; Lecture 11: Property Testing  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec50&#34;&gt; Property Testing Model  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec51&#34;&gt; Linearity Test by Blum, Luby and Rubinfield  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec52&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;#___sec53&#34;&gt; Lecture 12: Learning Theory  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec54&#34;&gt; Probably Approximately Correct!  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec55&#34;&gt; Learning Rectangles  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec56&#34;&gt; Learning CNFs  &lt;/a&gt;&lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;#___sec57&#34;&gt; Readings  &lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec0&#34;&gt;Lecture 1: Introduction &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;p&gt;
You have seen algorithms for a variety of problems. This begs the question
of &lt;em&gt;whether there is an algorithm for any  given problem?&lt;/em&gt;. 
Furthermore &lt;em&gt;is there an efficient 
algorithm for all problems?&lt;/em&gt;. Even if you have an algorithm for the problem, 
&lt;em&gt;can it be made more efficient?&lt;/em&gt;. This course will be addressing these 
questions. For answering these questions, we first need to define:

&lt;ol&gt;
   &lt;li&gt; What is a &lt;em&gt;problem&lt;/em&gt;?&lt;/li&gt;
   &lt;li&gt; What is computer/programing language, where you can execute/write 
      the algorithms?&lt;/li&gt;
   &lt;li&gt; What do you mean by &lt;em&gt;efficient&lt;/em&gt;?&lt;/li&gt;
&lt;/ol&gt;

We will start with very simplified definitions for these questions and 
try to find the answers for this setting.

&lt;h1 id=&#34;___sec1&#34;&gt;Decision Problems &lt;/h1&gt;

&lt;p&gt;
Typically the output of an algorithm could be a graph, a number etc. 
But we will consider only problems with an YES or NO answer. Furthermore,
we will define the problem to be just the set of all YES-instances. 
You are familiar with this definition from your automata theory class. 
For example take the REACHABILITY problem where the input is \( (G(V,E), s,t) \)
where \( s,t \in V \), and the problem is whether there is a path from \( s \) to 
\( t \) in the graph \( G \). So we will define the problem itself as the set of instances \( (G,s,t) \)
where a path exists.

$$
\mbox{REACHABILITY} = \{ (G,s,t) : \mbox{ there is a path in $G$ from } s \mbox{ to } t \}
$$

&lt;p&gt;
The number of instances of graphs having a particular number of vertices \( n \) 
is finite. But the set of all instances is the union of such instances with
\( n=1 \) to \( \infty \). Hence each instance has a &lt;em&gt;size&lt;/em&gt; \( n \) which is the 
number of vertices in the graph. Size of an instance is a fundamental concept,
and we will be defining it for all the instances. Later we will see, that size
plays an important role in defining what we mean by an efficient algorithm.

&lt;p&gt;
It was easy to define REACHABILITY because the original problem had an YES or
NO answer. But can we write problems for which the outputs are numbers, as 
decision problems? Lets consider the the problem of finding the &lt;em&gt;chromatic 
number&lt;/em&gt; of a graph. Chromatic number of a graph \( G \) is the minimum number of
distinct colors required to color a graph such that for every edge, end points
are of different colors. Note that for a graph on \( n \) vertices, the chromatic number is \( \leq n \).

&lt;p&gt;
The descision version of the chromatic number problem  takes the graph \( G \) and a number \( k \) between \( 1 \) and \( n \) as input. The output is YES, if the chromatic number is \( \leq k \). So 

$$
\text{CHROMATIC-NUMBER} = \{ (G,k) : \text{ chromatic number of graph } G \text{ is } \leq k \}
$$

&lt;p&gt;
Suppose you have an algorithm that takes \( T(n) \) steps for deciding the CHROMATIC-NUMBER problem, 
you can find the chromatic number also. For this we will create \( n \) instances of CHROMATIC-NUMBER
from the input graphs, setting \( k=1 \) to \( n \). Note that these instances will transition from NO-instances
to YES-instance and remain YES-instances. The chromatic number is the least \( k \) for which the instance
is an YES-instance. Hence you have algorithm which takes \( n\times T(n) \) steps for finding the exact 
chromatic number.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 1: &lt;/b&gt; Can you find a algorithm, which takes \( (\log n) T(n) \) steps?

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec2&#34;&gt;Turing Machines &lt;/h1&gt;

&lt;p&gt;
The simplfied model for computer/programming language will be the Turning Machine (TM).
The discovery of the TM has a long and interesting history. See &lt;a href=&#34;https://www.amazon.in/Logicomix-Epic-Search-Trurh-Truth/dp/0747597200/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1506918619&amp;sr=1-1&amp;keywords=logicomix&#34; target=&#34;_self&#34;&gt;Logicomix&lt;/a&gt;,
for a nice read in comic book format.

&lt;p&gt;
The Turing Machine has many tapes (which is similar to the hard disk in 
a modern computer) and a limited set of states (which is similar to the RAM or the cache).
There is a read-only input tape, where the input is written and a write-only output
tape where the output is expected when machine is finished. There is also multiple
work tapes, which can be used for storing intermediate results in the computation.
The work tapes are assumed to be of infinite length. This is needed because as 
the size of the input instances becomes larger, the intermediate memory also needs to 
be large. The set of states of the TM is fixed and does not change with input size.

&lt;p&gt;
The input is assumed to be a string from a finite alphabet \( \Sigma \). A tape cell
can store an additional blank symbol indicating that it is unused.

&lt;p&gt;
The algorithm is implemented in a TM by means of a state function. The state
function takes as arguments:

&lt;ol&gt;
   &lt;li&gt; the symbol under the heads of the TM in each of the tapes,&lt;/li&gt; 
   &lt;li&gt; well as the current state&lt;/li&gt; 
&lt;/ol&gt;

and returns 

&lt;ol&gt;
   &lt;li&gt; the next state&lt;/li&gt;
   &lt;li&gt; the symbols to be written under each of the work tape heads&lt;/li&gt;
   &lt;li&gt; the directions to move each of the heads one cell to the left, right or not move.&lt;/li&gt;
&lt;/ol&gt;

Now we define a TM using mathematical notation. \( \Sigma \) is the alphabet 
in which input and work tape is written (for eg. binary ie. \( \Sigma = \{0,1\} \)). 
The blank symbol for unwritten tape cell is \( \_ \). 
Let \( \Omega \) be the set of states of the TM. There are 2 special states of the
TM called \( \omega_{\text{start}} \) and \( \omega_{\text{stop}} \) called the start
and the stop states. The state transition function is denoted by 
$$
\delta : \Omega \times \Sigma \cup \{ \_ \} \rightarrow \Omega \times \Sigma^k \times \{ &lt; , &gt; , - \}^{k+2}
$$

&lt;p&gt;
Now lets see an example of a TM for solving the following problem:
$$
PALINDROMES = \{ x \in \{0,1\}^n : x \text{ is a palindrome}\}
$$

&lt;p&gt;
First lets try to write a pseudo code for a TM solving the problem:

&lt;ol&gt;
   &lt;li&gt; Copy the input in to the second tape&lt;/li&gt; 
   &lt;li&gt; Move the head in the input tape to end and second tape to begining&lt;/li&gt;
   &lt;li&gt; Compare each bit by moving the input head from end to beggining and second head from beggining to end&lt;/li&gt;
   &lt;li&gt; If any bit is different reject&lt;/li&gt;
   &lt;li&gt; If all bits match accept&lt;/li&gt;
&lt;/ol&gt;

We will use a turing machine simulator for implementing this. 
See the example : &lt;a href=&#34;http://turingmachinesimulator.com/shared/oihvkhvacu&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;http://turingmachinesimulator.com/shared/oihvkhvacu&lt;/tt&gt;&lt;/a&gt;

&lt;p&gt;
Note that it is extremely tedious to write algorithms as a TM. Neverthless, all the 
algorithms can be written as TM.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 2: &lt;/b&gt; Design a TM which decides the language :
$$
DIV5 = \{ x \in \{0,1\}^n : x \text{ is binary number divisible by } 5\}
$$

&lt;p&gt;
Write the program first as a pseudo code and then in the format in &lt;a href=&#34;https://turingmachinesimulator.com/.&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;https://turingmachinesimulator.com/.&lt;/tt&gt;&lt;/a&gt;

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
We will use a word &#34;decidable&#34; to indicate that a descision problem is solvable.
A TM is said to decide a descision problem if there is a TM, that accepts
all the YES-instances and rejects all other strings. Note that this TM will never 
go  into an infinite loop.

&lt;h2 id=&#34;___sec3&#34;&gt;Robustness of TM &lt;/h2&gt;

&lt;p&gt;
We will now see that doing some modifications to the TM definitions, does not change what it can compute. We will not be going into details, but only sketch the proof.

&lt;p&gt;
&lt;b&gt;Convert \( k \) tape TMs to single tape TMs.&lt;/b&gt;

&lt;p&gt;
We will merge the \( k \) tapes in to a single tape which has an alphabet that encodes all the \( k \) symbols. But we will also need to encode whether the head in the \( k \) tape TM was placed over a symbol. So if the orignal alphabet was \( \{0,1\} \) then the new alphabet will be \( \{0,1, \dot 0, \dot 1 \}^k \).

&lt;p&gt;
Each step of the \( k \) tape TM will be simulated by the one tape TM, by doing a full pass over its tape, reading all the alphabets with \( \{\dot 0, \dot 1\} \). Then it will erase and the write the dots according to the appropriate tape movement (given by the transition function of the old TM).

&lt;p&gt;
&lt;b&gt;Convert alphabet from any \( \Sigma \) to binary.&lt;/b&gt;

&lt;p&gt;
Suppose we have a TM with alphabet size \( |\Sigma| = t \). We will encode the \( t \) symbols into binary strings of length \( \log t \). For each step of the old TM, the new TM will go over \( \log t \) positions in its binary tape to read the symbol, and update \( \log t \) positions for writing the new symbol according to the transition funciton of the old TM.

&lt;p&gt;
It is important to note that, while doing these transformations, the alphabet size \( |\Sigma| \) and the state size \( |\Omega| \) remain constant, independent of the input size.

&lt;h1 id=&#34;___sec4&#34;&gt;Efficient Algorithms &lt;/h1&gt;

&lt;p&gt;
Recall that for every instance of a problem, we will have a size defined (denoted by \( n \)).
The number of steps taken by the TM to decide an instance will grow with \( n \).
Hence the running time of a problem will be defined in terms of the size \( n \).
Futhermore we will be considering &lt;em&gt;worst case&lt;/em&gt; running time. That is the running time
for intances of size \( n \) will be defined as the the largest running time,
among all intances of size \( n \).

&lt;p&gt;
For example, consider the Dijkstra&#39;s algorithm for 
shortest path. There are instances of graph of size \( n \) like a 
\( n \) length path on which the algorithm takes only \( O(n) \) steps. However if the 
graph is a complete graph, it takes \( O(n^2) \) steps. But it never takes more than
\( O(n^2) \) steps on instances of size \( n \). Hence running time of Dijkstra&#39;s
algorithm will be considered as \( O(n^2) \).

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 3: &lt;/b&gt; Let \( f(n) \) and \( g(n) \) be any two of the following functions. Determine whether
\( f(n) = O(g(n)) \),  \( f(n) = \Omega(g(n)) \) and \( f(n) = \Theta(g(n)) \).

&lt;ol&gt;
   &lt;li&gt; \( n! \)&lt;/li&gt;
   &lt;li&gt; \( n^{\log n} \)&lt;/li&gt;
   &lt;li&gt; \( n^2 \) when \( n \) is odd and \( 2^n \) otherwise&lt;/li&gt;
   &lt;li&gt; \( n^3 \)&lt;/li&gt;
   &lt;li&gt; \( n^{n} \)&lt;/li&gt;
   &lt;li&gt; \( 2^n \)
            &lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;___sec5&#34;&gt;Polynomial time vs Exponential Time &lt;/h2&gt;

&lt;p&gt;
Now lets consider the CHROMATIC-NUMBER problem. There is very simple brute force
algorithm for it. Given an instance \( (G,k) \), for every assignment of \( k \) colors to 
the vertices, check for every edge, that the end points have different colors. If we 
find a color assignment which passes all checks, the TM can say YES. If for all 
assignments some check fails, then the TM says NO. This is a valid algorithm. Note
that for a NO-instance, the TM has to loop over all the color assignments which is
\( k^n \) in number, and do all the checks for each edge. Hence the running time is \( O(k^n\times n^2) \).

&lt;p&gt;
Note that the running time for the above algorithm grows exponentialy in \( n \). Even if
\( n=100 \), the running time is a very huge number that even a fast computer cannot hope
to solve it. So now the question is where there is another algorithm for CHROMATIC-NUMBER,
which is faster.

&lt;p&gt;
Our definition of efficient algorithms will be &lt;em&gt;polynomial time algorithms&lt;/em&gt;, that is 
algorithms for which the running time is of the form \( O(n^t) \) for some fixed integer \( t \).

&lt;h1 id=&#34;___sec6&#34;&gt;Readings &lt;/h1&gt;

&lt;p&gt;
&lt;b&gt;Chapter 3.&lt;/b&gt;

Michael Sipser, Introduction to Theory of Computation

&lt;p&gt;
&lt;b&gt;Chapter 1, Chapter 2.&lt;/b&gt;
Christos Papadimitriou, Computational Complexity

&lt;p&gt;
&lt;b&gt;Misc: &lt;/b&gt;
&lt;a href=&#34;https://www.amazon.in/Logicomix-Epic-Search-Trurh-Truth/dp/0747597200/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1506918619&amp;sr=1-1&amp;keywords=logicomix&#34; target=&#34;_self&#34;&gt;Logicomix&lt;/a&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec7&#34;&gt;Lecture 2: Paradox&#39;s and Diagonalization &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec8&#34;&gt;Russell&#39;s Paradox and Diagonalization &lt;/h1&gt;

&lt;p&gt;
&lt;em&gt;There is a village where the barbers shave only those people who 
do not shave by themselves. Now does a barber shave himself?&lt;/em&gt;

&lt;p&gt;
If YES, then he cannot shave himself. If NO, then he can shave himself.

&lt;p&gt;
See &lt;a href=&#34;https://www.amazon.in/Godel-Escher-Bach-Eternal-Golden/dp/0140289208&#34; target=&#34;_self&#34;&gt;Godel, Escher, Bach&lt;/a&gt;,
for more paradoxes and their history.

&lt;p&gt;
Now we will solve a problem using the Russell&#39;s paradox. 
Let \( \mathbb N \) be the set of natural numbers and \( \mathcal{P}(N) \) is the set of all
subsets of \( \mathbb N \) (or the power set of \( \mathbb N \)). Show that there cannot 
be a one to one mapping between \( \mathbb N \) and \( \mathcal{P}(N) \) that has range 
equal to \( \mathcal{P}(N) \) (that is for every subset of \( \mathbb N \), there is some 
integer that maps to it).

&lt;p&gt;
Suppose there is a one to one mapping \( f \) that has range 
equal to \( \mathcal{P}(N) \). Consider the set 
$$
S = \{ x : x \notin f(x) \}
$$

Since \( f \) that has range equal to \( \mathcal{P}(N) \), there is some 
integer \( y \) that maps to \( S \) (that is \( f(y) = S \)). Now does \( y \in S \)?

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 4: &lt;/b&gt; Let STRINGS be the set of all infinite length binary sequences. That is 
$$
\text{STRINGS} = \{ x:\mathbb{N} \rightarrow \{0,1\}  \}
$$

That is any infinite length binary sequence is simply a function that maps every integer 
to the bit at the corresponding position. Show that there cannot 
be a one to one mapping between \( \mathbb N \) and STRINGS.

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec9&#34;&gt;Universal Turing Machines &lt;/h1&gt;
Computer as we know them are general purpose machines. That is, you can give them
any program in a particular format (a programming langauge, or assembly), and they
can execute it. However the Turing Machines that we saw in the previous lecture,
seems to be tailor made for a particular problem.

&lt;p&gt;
The first step to making Turing Machines general purpose is to encode a TM as a string,
which could be fed as input to a general purpose TM, which we will call Universal Turing
Machines. As we saw previously, a TM is a tuple 

$$
M = (\Omega, \Sigma, \mathcal{T}, \delta, \omega_{\text{start}}, \omega_{\text{accept}}, \omega_{\text{reject}})
$$

&lt;p&gt;
where \( \Omega \) is a set of states, \( \Sigma \) is the alphabet for the language, \( \mathcal{T} \) is the tape 
alphabet, \( \delta \) is a transition function and \( \omega_{\text{start}}, \omega_{\text{accept}}, \omega_{\text{reject}} \)
are the special start, reject, accept states respectively. Note that the states, the alphabets and the special states,
are all finite and can be encoded as a finite alphabet. \( \delta \) function also is finite and could be represented as 
a set of rules (similar to how the turning machine simulater is doing). We will denote the string representing a
TM \( M \) by \( \langle M \rangle \).

&lt;p&gt;
A Universal Turing Machine takes a string \( (\langle M \rangle, x) \), and simulates the running of the TM \( M \) on the 
input \( x \). It is just another TM having its own transition function, alphabets and states. However it is designed
such that it can accept a TM encoding \( \langle M \rangle \) as in part of the input, and the transition function
is defined such that it simulates the running of \( M \) on a string \( x \).

&lt;p&gt;
&lt;b&gt;Configuration.&lt;/b&gt;
An important concept for doing simulation is the configuration of a TM. Lets 
stick to 1-tape TMs for now. The &#34;state&#34; of the algorithm (not the TM state) is
really consist of the the TM&#39;s state, the contents of its tape as well as the position
of the tape heads. If a TM is at state \( \omega_i \), has the string 101101111 in 
its tape and the tape head is at the 5th position, then its current configuration is 
1011 \( \omega_i \) 01111. Note that this is a string over the alphabet \( \mathcal{T} \cup \Omega \). 
The computation of TM is a graph in the space of all TM configurations. If the TM 
halts, it is a path which ends in a configuration having an accept or reject state.
If it loops, there will be a cycle in this graph.

&lt;p&gt;
A universal TM, goes over a configuration string, then goes over the encoding of the delta function,
finds out which rule in the delta function to use, and can update the configuration,
to reflect the next configuration of the TM begin simulated.

&lt;h1 id=&#34;___sec10&#34;&gt;Halting Problem &lt;/h1&gt;

&lt;p&gt;
Recall that we started with the question whether all problems can be solved by a TM.
Consider the following descision problem:
$$
H = \{ (\langle M \rangle,x) : M \text{ accepts } x \}
$$

That is, given a TM encoding \( \langle M \rangle \) and a string \( x \), check if 
\( M \) accepts \( x \). Can you think of a TM which decides \( H \). One way is to simulate \( M \) on \( x \)
like the universal TM. But its possible that \( M \) loops on \( x \) and the UTM will also loop.

&lt;p&gt;
Suppose there a TM \( M_H \) which decides \( H \). We will define a new TM \( D \) which takes an encoding
of a TM \( \langle M \rangle \) as input , which does
the following 

&lt;ol&gt;
   &lt;li&gt; Run \( M_H \) on \( (\langle M \rangle, \langle M \rangle) \).&lt;/li&gt;
   &lt;li&gt; Output the opposite of what \( M_H \) outputs.&lt;/li&gt;
&lt;/ol&gt;

So \( D \) accepts \( \langle M \rangle \) if \( M \) rejects the input \( \langle M \rangle \) and 
it rejects of if \( M \) accepts the input \( \langle M \rangle \). Now the question is 
what is the output of of \( D \) when run on \( \langle D \rangle \)?

&lt;p&gt;
You will see that there is a problem, very similar to the Russell&#39;s paradox here. Hence 
there cannot be a TM \( D \). But we can construct \( D \) if there is a TM \( M_H \). So 
neither can there be a TM \( M_H \) which decides \( H \).

&lt;p&gt;
\( H \) is popularly known as the halting problem which was shown by Turing to be not
decidable (or undecidable).

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 5: &lt;/b&gt; Let \( \text{EMPTY} = \{ \langle M \rangle : M \text{ rejects all inputs } \} \). Show 
that this problem is undecidable.

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec11&#34;&gt;Lecture 3: Non Determinism, NP and Search Problems &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec12&#34;&gt;Non Deterministic Turing Machines &lt;/h1&gt;

&lt;p&gt;
Last lecture we saw that TM&#39;s can be encoded as strings and simulated by a
universal TM using the configurations. We also saw that simulation of a TM,
is essentialy tracing a path in the configuration space. But in the TMs that 
we defined, the out degree of any node (a configuration) 
in the graph (in the configuration space) is one.

&lt;p&gt;
Similar to the Nondeterministic finite automata, we can also define TMs 
with delta rules that results in multiple next states 
(called Nondeterministic Turing Machines or NTMs). Then the delta rules
will be of the form \( \delta:\Omega\times \mathcal{T} \rightarrow \mathcal{P}(\Omega\times \mathcal{T}\times \{\langle, \rangle, -\}) \).
(\( \mathcal{P} \) denotes the power set). So the configuration graph of a 
NTM can have out degree greater than \( 1 \). However the it still has to be a finite 
number since the size of \( \mathcal{P}(\Omega\times \mathcal{T}\times \{\langle, \rangle, -\}) \) is 
finite. Following the different paths, an NTM could accept, reject or keep looping.
So we need to define what is meant by deciding a language by an NTM.

&lt;p&gt;
An NTM is said to decide a language (a descision problem) \( L \) iff,
for all strings in the language, there exists one path in the configuration 
space that results in accept state.  For strings not in the language, all 
paths in the configuration space should result in reject state.

&lt;p&gt;
We know that NFA (Nondeterministic Finite automata) can always be converted to 
a deterministic one. However for Pushdown Automaton this coverstion is not possible
always. We will see that for TMs, this conversion can always be done. That is the 
set of languages that can be decided by TMs does not change by allowing 
nondeterminism.

&lt;p&gt;
Recall that UTM simulated a deterministic TM, by tracing the path in the configuration
space. But for NTMs, the graph in the configuration space is a tree. A simple 
idea is for a UTM to do a graph traversal. DFS might be a bad idea, because
some of the paths go into infinite loops. Hence it can to BFS. The first time,
it finds that the NTM has reached the accept state, the UTM can also accept.
If it never finds an accept state, the simulating TM rejects.

&lt;h1 id=&#34;___sec13&#34;&gt;Nondeterministic Polynomial Time : NP &lt;/h1&gt;

&lt;p&gt;
As we disscussed earlier, an NTM can take different paths in the configuration space.
The length of the path is essentialy the number of steps. Now we will define 
the worst case running time for a NTM.

&lt;p&gt;
For a language \( L \), on inputs of size \( n \), the worst case running time of an NTM 
is the length of the longest path in the configuration space on any of the inputs 
of size \( n \).

&lt;p&gt;
NP or Nondeterministic Polynomial time  is the class of descision problems for which,
there is an NTM which decides it in worst case polynomial time.

&lt;h1 id=&#34;___sec14&#34;&gt;NP : Verifier Definition &lt;/h1&gt;

&lt;p&gt;
We will define descision problems that are &lt;em&gt;verifiable&lt;/em&gt;. A descision problem 
is said to be verifable if there exists  a deterministic TM \( M \) that takes two 
inputs \( (x,y) \) where \( x \) is an instance of the descision problem and \( y \) is
called a certificate which has length of  atmost \( p(n) \) where \( p \) is a polynomial. 
For YES-instances \( x \), there should exist a certificate 
\( y \) such the \( M \) accepts the input \( (x,y) \). For instances not in the language,
for every \( y \) of length \( p(n) \), \( M \) should reject on \( (x,y) \). Also the running
time of \( M \) must be polynomial time in size of \( x \).

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 6: &lt;/b&gt; Show that NP is the same as the set of verifable languages.

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
An example of a verifiable language is the Clique problem.

$$
\text{CLIQUE} = \{ (G,k): G \text{ has a clique of size } k  \}
$$

&lt;p&gt;
A certificate for YES-instance \( (G,k) \),  is just the list of vertices in a \( k \)-clique.
The polynomial time verifier just checks if there is an edge between all pairs of
vertices. If \( (G,k) \) is not in the language, then for any set of \( k \) vertices,
that you can give to the verifier, it will reject, since there will not be an 
edge between some pair in the list.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 7: &lt;/b&gt; Show that the CHROMATIC-NUMBER problem defined in Lecture 1 is verifiable.

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec15&#34;&gt;Descision vs Search Problems &lt;/h1&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec16&#34;&gt;Lecture 4: Reductions, Cook-Levin Theorem and NP-Completeness &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;p&gt;
The notes are mostly from Section 7.4, 7.5 in the Sipser book.

&lt;h1 id=&#34;___sec17&#34;&gt;Polynomial Time Reductions &lt;/h1&gt;

&lt;p&gt;
Last lecture, we showed that for some problems, the search problem can be solved in polytime,
if the corresponding desciion problem can be solved in polytime. In this lecture,
we will show that many desicion problems can be solved by solving one particular 
desciion problem called \( 3 \)-SAT.

&lt;p&gt;
For this we need first define a reduction. The following problems were defined previously

&lt;ol&gt;
   &lt;li&gt; \( 3 \)-SAT = \( \{ \phi : \phi \text{ is a 3CNF formula that is satisfiable} \} \).&lt;/li&gt;
   &lt;li&gt; CLIQUE = \( \{ (G,k) : G \text{ has a clique of size } k \}. \)&lt;/li&gt;
&lt;/ol&gt;

We will say that \( f \) is a (poly time) reduction from \( 3 \)-SAT to CLIQUE, if it maps \( 3 \)-CNF formulas \( \phi \)
to a tuple \( (G,k) \) where \( G \) is a graph and \( k \) is a number such that 

&lt;ol&gt;
   &lt;li&gt; \( f \) can be computed by a polynomial time TM.&lt;/li&gt;
   &lt;li&gt; \( \phi \) is satisfiable if and only if \( G \) has a \( k \) clique.&lt;/li&gt;
&lt;/ol&gt;

If we have such an \( f \), any polynomial time algorithm for CLIQUE can be used to 
design a polynomial time algorithm for \( 3 \)-SAT.

&lt;p&gt;
Verify that: If we have such an \( f \), give a polynomial time algorithm for \( 3 \)-SAT, 
assuming there is a polynomial time algorithm for CLIQUE.

&lt;p&gt;
The algorithm for computing \( f \) is as follows:

&lt;ol&gt;
   &lt;li&gt; For every clause in \( \phi \), put \( 3 \) new verticies correponding to each literal in the clause.&lt;/li&gt;
   &lt;li&gt; Put all edges in the graph except:

&lt;ol type=&#34;a&#34;&gt;&lt;/li&gt;
       &lt;li&gt; between the 3 veritices correponding to the same clause.&lt;/li&gt;
       &lt;li&gt; \( x_i \) an \( \bar x_i \) for all \( i \).&lt;/li&gt;
&lt;/ol&gt;

   &lt;li&gt; Set \( k \) to be equal to the number of clauses.&lt;/li&gt;
&lt;/ol&gt;

Verify the following:

&lt;ol&gt;
   &lt;li&gt; \( f \) is polynomial time.&lt;/li&gt;
   &lt;li&gt; Show that if \( \phi \) is satisfiable \( G \) has a \( k \) CLIQUE.&lt;/li&gt;
   &lt;li&gt; Show that if \( G \) has a \( k \) CLIQUE then \( \phi \) is satisfiable.&lt;/li&gt;
&lt;/ol&gt;

Such a reduction says that CLIQUE is a harder problem than \( 3 \)-SAT, because an algo for 
CLIQUE gives an algo for \( 3 \)-SAT and we dont know if the reverse is True. Hence it is denote as
$$ \text{CLIQUE} \geq_p 3\text{-SAT}.$$

&lt;h1 id=&#34;___sec18&#34;&gt;Cook-Levin Theorem &lt;/h1&gt;
The Cook-Levin Theorem tells that the reverse reduction also exists. In fact,
it states that any language in NP can be reduced to SAT

&lt;p&gt;
&lt;b&gt;Cook-Levin Theorem.&lt;/b&gt;
For any language \( L \in NP \), SAT \( \geq_p  \) L.

&lt;p&gt;
For doing this, for any language \( L \), that has a nondeterministic polynomial time TM,
we need to come up with a polynomial time reduction, which satisfies the conditions,
given in the previous section.

&lt;p&gt;
See Theorem 7.37 for the proof of Cook-Levin Theorem.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 8: &lt;/b&gt; Show that \( 3 \)-SAT \( \geq_p \) SAT.

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec19&#34;&gt;NP Compeleness / Hardness &lt;/h1&gt;

&lt;p&gt;
Vertex Cover 3SAT Reduction.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 9: &lt;/b&gt; Solve Problem 7.21 in Sipser book

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 10: &lt;/b&gt; Solve Problem 7.23 in Sipser book

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 11: &lt;/b&gt; Solve Problem 7.27 in Sipser book

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec20&#34;&gt;Lecture 5: More Time Complexity &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec21&#34;&gt;NP-completeness of Vertex Cover and Subset Sum &lt;/h1&gt;
See Section 7.5 in Sipser 2nd Edition

&lt;h1 id=&#34;___sec22&#34;&gt;EXP and Time Heirachy Theorem &lt;/h1&gt;
EXPTIME is the set of languages for which there is a poly time determinitic
TM that runs in \( 2^{n^k} \) for some integer \( k \).

&lt;p&gt;
Clearly \( \text{P} \subseteq \text{NP} \subseteq \text{EXPTIME} \).

&lt;p&gt;
As we discussed, the question if P=NP or P \( \subsetneq \) NP is an open problem.
But what about P \( \subsetneq \) EXP?

&lt;p&gt;
This is easy to prove using diagonalization. Let DTIME(\( f(n) \)), be the 
set of languages that can be decided in time \( f(n) \) by a determinitic TM.
Note that P \( \subseteq \) DTIME(\( 2^n \)). We will design a language that is 
different from all languages in DTIME(\( 2^n \)) by can be decided in EXPTIME.

&lt;p&gt;
We define the language by giving a TM \( D \). \( D \) takes encodings of TMs as 
input \( \langle M \rangle \). D simulates M on \( \langle M \rangle \) for \( 2^n \) steps. 
If M halts in \( 2^n \) steps, it gives the opposite answer. Otherwise it rejects.
 Due to 
the overheads involved in simulating a TM, the TM \( D \) will take more time 
than \( 2^n \), however halts in less than \( 2^{n^2} \) steps. Hence the 
language decided by \( D \) is in EXPTIME.

&lt;p&gt;
But can the language decided by \( D \) be in DTIME(\( 2^n \)). That is their 
another TM \( D&#39; \) that decided this language in \( 2^n \) steps. If so 
what is the output of \( D \) on the input \( \langle D&#39; \rangle \).

&lt;p&gt;
You can see that this results in a contradiction and hence the 
language decided by \( D \) is not in P. So P \( \subsetneq \) EXPTIME.

&lt;p&gt;
The question of whether NP = EXPTIME is again open.

&lt;h1 id=&#34;___sec23&#34;&gt;coNP and Map of Complexity classes  &lt;/h1&gt;
The complement of a language \( L \) is 
$$ \bar L = \{ x : x \not in L \}.$$ 
Let coNP \( = \{ L : \bar L \in \text{NP} \} \).

&lt;p&gt;
Another way of defining coNP is: it is the set of languages \( L \) for which
there is a determinitic TM \( M \) that takes 2 inputs \( x,y \) such that 

&lt;ol&gt;
   &lt;li&gt; If \( x \in L \) then for every \( y \), \( M(x,y) \) accepts.&lt;/li&gt;
&lt;/ol&gt;

    p If \( x \notin L \) then there exist a \( y \), for which \( M(x,y) \) rejects.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 12: &lt;/b&gt; Show that these two definitions gives the same set of languages.

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
coNP is a set of languages for which there is a certificate for non membership.
Similar to NP-complete, we can define:
$$\text{coNP-complete} = \{ L \in \text{coNP} : \forall L&#39; \in \text{coNP}, L&#39; \leq_p L \}$$

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 13: &lt;/b&gt; Show that UNSAT = \( \{ \phi : \phi \text{ is a boolean formula that is unsatisfiable} \} \),
is coNP-complete
(a formula is unstatisfiable when for all assignments to the variable, the formula evaluates to False).

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
Note that P = coP.

&lt;p&gt;
Map of complexity classes

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec24&#34;&gt;Lecture 6: Space Complexity &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec25&#34;&gt;Space Complexity classes and some Relationships &lt;/h1&gt;

&lt;p&gt;
Let SPACE$(f(n))$ be the set of languages that can be decided in space \( f(n) \),
then 
$$\text{TIME}(f(n)) \subseteq \text{SPACE}(f(n)) \subseteq \text{TIME}(2^{cf(n)}).$$

&lt;p&gt;
Let coSPACE$(f(n))$ be the complements of languages in SPACE$(f(n))$, then
$$\text{coSPACE}(f(n)) = \text{SPACE}(f(n)).$$

&lt;p&gt;
Let L be the set of languages that can be decided in space \( c\log n \) for some constant \( c \),
and PSPACE the set of languages that can be deciding in polynomial space. Then 
$$ \text{L} \subsetneq \text{PSPACE}.$$

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 14: &lt;/b&gt; Show that 
   &lt;li&gt; \( \text{L} = \text{coL} \subseteq \text{P} \subset \text{PSPACE} \subseteq \text{EXPTIME}. \)&lt;/li&gt;
   &lt;li&gt; \( \text{L} \subsetneq \text{PSPACE}. \)

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec26&#34;&gt;Non Determinism and Space : NL-complete, Savith&#39;s Theorem &lt;/h1&gt;

&lt;p&gt;
Just like we defined NP, we can define NL as the set of languages that
is decided by a non deterministic log-space TM. Whether L = NL is again 
a well known open problem like P = NP. We can do reductions in NL. 
As we will see later \( NL \subseteq P \). Therefore any two languages 
in NL are polynomial time reducible (in fact a polytime machine solve the problem 
itself). Hence for defining NL-complete langauges, we use log-space reductions
that runs in \( O(\log n) \) space (denoted by \( \leq_l \)). That is 
$$\text{NL-complete} = \{ R : R \text{ is a language such that for all other languages R&#39; } \in L, R&#39; \leq_l R \}.$$

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 15: &lt;/b&gt; Show that&lt;/li&gt; 
   &lt;li&gt; \( \text{REACHABILITY} \in \text{NL-complete}. \)&lt;/li&gt;
   &lt;li&gt; \( \text{NL} \subseteq \text{P}. \)

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
Actually all languages is NL can be decided by a deterministic \( O(\log^2 n) \) space TM. 
This theorem is known as Savitch&#39;s Theorem.

&lt;p&gt;
Savitch&#39;s theorem says that \( \text{NPSPACE}(f(n)) \subseteq \text{SPACE}(f^2(n)) \).
See proof in proof of Theorem 8.5 in Sipser book.

&lt;h1 id=&#34;___sec27&#34;&gt;Overview of next part of course  &lt;/h1&gt;

&lt;p&gt;
Streaming algorithms for Well paranthesised expressions.
Lowerbound using communication complexity.

&lt;h1 id=&#34;___sec28&#34;&gt;Readings  &lt;/h1&gt;
Chapter 8, Introduction to Theory of Computation by Micheal Sipser, Edition 2.

&lt;p&gt;
Chapter 4, Computational Complexity: A Modern Approach
Sanjeev Arora and Boaz Barak
http://theory.cs.princeton.edu/complexity/book.pdf

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec29&#34;&gt;Lecture 7: NL = coNL, Randomization &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec30&#34;&gt;Recapping Nondeterminism &lt;/h1&gt;

&lt;p&gt;
When do we say a Nondeterministic TM accepts a languages?

&lt;p&gt;
Why is it not obiviously true that \( NP = coNP \).?

&lt;p&gt;
Verifier definition of NP, NL.

&lt;h1 id=&#34;___sec31&#34;&gt;Immerman-Szelepcsenyi Theorem : NL = coNL &lt;/h1&gt;

&lt;p&gt;
In time complexity, we didnt have a proof of NP = coNP. This remains an open problem.
However in space complexity, we know that NL = coNL. This theorem is known as the 
Immerman-Szelepcsenyi Theorem.
See Section 8.6 in Sipser book.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 16: &lt;/b&gt; Given an NL algorithm (use verifier definition) :&lt;/li&gt;
   &lt;li&gt; which on input a graph \( G(V,E) \), vertices \( s,t \in V \) and a number \( i \), checks if \( s \) is reachable from \( t \) in \( \leq i \) steps.&lt;/li&gt;
   &lt;li&gt; which on input a graph \( G(V,E) \), verticex \( s \in V \) and a numbers \( i, c \), checks if the number of vertices reachable from \( s \) in \( i \) steps is  \( \geq c \).&lt;/li&gt;
   &lt;li&gt; which on input a graph \( G(V,E) \), verticex \( s \in V \) and a numbers \( i, c \), checks if the number of vertices not reachable from \( s \) in \( i \) steps is  \( =c \).

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec32&#34;&gt;Randomized TMs and Complexity Classes &lt;/h1&gt;

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 17: &lt;/b&gt; Show that&lt;/li&gt;
   &lt;li&gt; RP \( \subseteq \) BPP&lt;/li&gt;
  &lt;li&gt; coRP \( \subseteq \) BPP&lt;/li&gt;
  &lt;li&gt; BPP \( \subseteq \) PSPACE&lt;/li&gt;
  &lt;li&gt; RP \( \subseteq \) NP&lt;/li&gt;
  &lt;li&gt; BPP \( = \) coBPP

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 18: &lt;/b&gt; &lt;em&gt;Bonus Problem&lt;/em&gt;
A forest is a graph which is a disjoint union of trees (ie tree on different set of vertices).
Show that REACHABILITY in a forest can be solved using \( O(\log n) \) space.

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec33&#34;&gt;Readings  &lt;/h1&gt;
Chapter 8, Introduction to Theory of Computation by Micheal Sipser, Edition 2.

&lt;p&gt;
Chapter 4, Computational Complexity: A Modern Approach
Sanjeev Arora and Boaz Barak
http://theory.cs.princeton.edu/complexity/book.pdf

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec34&#34;&gt;Lecture 8: Randomized Algos &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec35&#34;&gt;BPP and Amplification  &lt;/h1&gt;

&lt;p&gt;
Last lecture we saw that \( \text{RP}_{1/100} = \text{RP}_{99/100} \). 
This lecture we said that \( \text{BPP}_{1/2 + \epsilon} = \text{BPP}_{1 - \epsilon} \)
for any small constant \( \epsilon \).

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 19: &lt;/b&gt; Let \( \text{BPP}_\alpha \) be the class of languguages L that have a PTM M such that&lt;/li&gt;
  &lt;li&gt; If \( x\in L \) then \( \Pr[M(x) \text{accepts}] \geq \alpha \).&lt;/li&gt;
  &lt;li&gt; If \( x\notin L \) then \( \Pr[M(x) \text{rejects}] \geq \alpha \).

&lt;p&gt;
Show that:&lt;/li&gt;

&lt;ol&gt;
   &lt;li&gt; \( \text{BPP}_{1/2} \) is the set of all languages.&lt;/li&gt;
   &lt;li&gt; \( \text{BPP}_{2/3} = \text{BPP}_{20/27} \) (without using Chernoff&#39;s Bounds).
            &lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

See Chapter 10.2 in Sipser

&lt;h1 id=&#34;___sec36&#34;&gt;Approx MAX-CUT  &lt;/h1&gt;
See Chapter 10.1 in Sipser for approximation algorithms. 
See Section 2.3.4 in 
https://people.seas.harvard.edu/~salil/pseudorandomness/power.pdf

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 20: &lt;/b&gt; The Approximate-7/8-3SAT problem is given an input a \( 3 \)-SAT boolean formula,
find an assignment that satisfies atleast \( 7/8 \) fraction of the clauses (ie if 
there are \( m \) clauses, find an assignment that satisfies \( m\times 7/8 \) clauses).

&lt;p&gt;
Give a randomized algorithm which solves Approximate-7/8-3SAT (answer just need
to be correct in expectation as we discussed for the MAXCUT problem in class.).

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec37&#34;&gt;Undirected REACHABILITY in RL &lt;/h1&gt;

&lt;p&gt;
See Section 2.4 in 
https://people.seas.harvard.edu/~salil/pseudorandomness/power.pdf

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 21: &lt;/b&gt; Let M be the adjacency matrix of a undirected \( d \)-regular graph \( G \) (all vertices have exactly \( d \) neighbours).
Show that:

&lt;ol&gt;
   &lt;li&gt; \( d \) is an eigenvalue of \( M \).&lt;/li&gt;
   &lt;li&gt; If \( c \) is the number of connected components in \( G \), then there are \( c \) linearly independent eigenvectors of \( M \) with eigen value \( d \).&lt;/li&gt;
   &lt;li&gt; If \( G \) is a bipartite graph then \( -d \) is an eigenvalue of \( M \).
            &lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 22: &lt;/b&gt; &lt;b&gt;Bonus Problem&lt;/b&gt;
Let M be the adjacency matrix of a undirected \( d \)-regular graph \( G \) (all vertices have exactly \( d \) neighbours).
Show that:

&lt;ol&gt;
   &lt;li&gt; If there are \( c \) linearly independent eigenvectors of \( M \) with eigen value \( d \) then \( G \) has \( c \) connected components.&lt;/li&gt;
   &lt;li&gt; If \( -d \) is an eigenvalue of \( M \) then \( G \) is a bipartite graph,.
            &lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;___sec38&#34;&gt;Readings  &lt;/h1&gt;

&lt;p&gt;
Chapter 10, Intro to Theory of Comp., Edition 2, Sipser

&lt;p&gt;
Chapter 2, Pseudorandomness survey,
Salil Vadhan 
https://people.seas.harvard.edu/~salil/pseudorandomness/power.pdf
https://people.seas.harvard.edu/~salil/pseudorandomness/

&lt;p&gt;
Chapter 7, Computational Complexity: A Modern Approach
Sanjeev Arora and Boaz Barak
http://theory.cs.princeton.edu/complexity/book.pdf

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec39&#34;&gt;Lecture 9: Random Walks, Derandomization and PIT  &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec40&#34;&gt;Analysis of Random Walks &lt;/h1&gt;
See Section 7.A in Arora, Barak book.

&lt;h1 id=&#34;___sec41&#34;&gt;Derandomizing MaxCUT &lt;/h1&gt;

&lt;p&gt;
For Method of conditional expectations see:
Chapter 3, Section 3.4 Pseudorandomness survey,
Salil Vadhan

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 23: &lt;/b&gt; Recall the Approximate-7/8-3SAT from Problem 20.
Derandomize the randomized algorithm which is 
the solution to Problem 20, using the method of conditional expectations

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec42&#34;&gt;Randomization: PIT and Schwartz-Zippel Lemma &lt;/h1&gt;
Polynomial written as a arithmetic circuit.

&lt;p&gt;
PIT problem

&lt;p&gt;
Schwartz-Zippel Lemma

&lt;p&gt;
Randomized algorithm for PIT

&lt;p&gt;
Chapter 2, Section 2.1 Pseudorandomness survey,
Salil Vadhan

&lt;h1 id=&#34;___sec43&#34;&gt;Readings  &lt;/h1&gt;

&lt;p&gt;
Chapter 2, Section 2.1 Pseudorandomness survey,
Salil Vadhan 
https://people.seas.harvard.edu/~salil/pseudorandomness/power.pdf

&lt;p&gt;
Chapter 3, Section 3.4 Pseudorandomness survey,
Salil Vadhan 
https://people.seas.harvard.edu/~salil/pseudorandomness/basic.pdf
https://people.seas.harvard.edu/~salil/pseudorandomness/

&lt;p&gt;
Section 7.A, Chapter 7, Computational Complexity: A Modern Approach
Sanjeev Arora and Boaz Barak
http://theory.cs.princeton.edu/complexity/book.pdf

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec44&#34;&gt;Lecture 10: Streaming Algorithms and Lowerbounds  &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec45&#34;&gt;Schwartz Zippel Lemma Proof  &lt;/h1&gt;

&lt;p&gt;
See Proof of Lemma A.25 in &lt;a href=&#34;http://theory.cs.princeton.edu/complexity/book.pdf&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;http://theory.cs.princeton.edu/complexity/book.pdf&lt;/tt&gt;&lt;/a&gt;

&lt;p&gt;
It is a simple proof by induction.

&lt;h1 id=&#34;___sec46&#34;&gt;Lowerbound for Bracket Matching  &lt;/h1&gt;

&lt;p&gt;
For strings \( x,y \in \{0,1\}^n \), we showed that any determinsitic streaming 
algorithm which given input \( x \cdot y \) checks if \( x = y \) will require \( n \) bits
of memory.

&lt;p&gt;
Same proof also holds for checking if \( x = \text{reverse}(y) \). Using a streaming reduction
we showed that for the same lowerbound holds for the bracket matching problem also.

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 24: &lt;/b&gt; Why is a streaming reduction required above?

&lt;p&gt;
Will using a log-space or polynomial time reduction result in the proof of the same lowerbound?

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 25: &lt;/b&gt; Given a string \( x \in \{0,1\}^n \), let the set represented by it be 
\( S_x = \{ i : x_i = 1 \} \).

&lt;p&gt;
Show that any deterministic streaming algorithm which on input \( x \cdot y \) (x,y are strings of length \( n \) ie \( \in \{0,1\}^n \)),
checks whether \( S_x \) and \( S_y \) are disjoint requires \( n \) bits of memory.

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec47&#34;&gt;Randomized Streaming Algorithm for Bracket Matching  &lt;/h1&gt;

&lt;p&gt;
See &lt;a href=&#34;https://www.dropbox.com/s/j7p2enoptwzky25/slides.pdf&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;https://www.dropbox.com/s/j7p2enoptwzky25/slides.pdf&lt;/tt&gt;&lt;/a&gt;?dl=0

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 26: &lt;/b&gt; The algorithm discussed in class (see the above slides), as mentioned in the slides 
only works for a restricted case of bracket matching (1-turn DYCK).

&lt;p&gt;
Give a counter example for the algorithm for the general bracket matching problem.

&lt;p&gt;
&lt;/div&gt;

&lt;p&gt;
 &lt;div class=&#39;problem&#39;&gt;
                &lt;b&gt;Problem 27: &lt;/b&gt; Give a randomized streaming algorithm (with one sided error) which on input \( x \cdot y \) (x,y are strings of length \( n \) ie \( \in \{0,1,2\}^n \)),
checks whether \( x=y \) which uses only \( O(\log n) \) bits of memory.

&lt;p&gt;
Show its correctness.

&lt;p&gt;
&lt;/div&gt;

&lt;h1 id=&#34;___sec48&#34;&gt;Readings  &lt;/h1&gt;

&lt;p&gt;

&lt;center&gt;&lt;h1 id=&#34;___sec49&#34;&gt;Lecture 11: Property Testing  &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec50&#34;&gt;Property Testing Model  &lt;/h1&gt;

&lt;h1 id=&#34;___sec51&#34;&gt;Linearity Test by Blum, Luby and Rubinfield  &lt;/h1&gt;

&lt;p&gt;
See Section 5.2 in &lt;a href=&#34;http://www.tcs.tifr.res.in/~prahladh/teaching/2009-10/limits/lectures/lec05.pdf&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;http://www.tcs.tifr.res.in/~prahladh/teaching/2009-10/limits/lectures/lec05.pdf&lt;/tt&gt;&lt;/a&gt;

&lt;h1 id=&#34;___sec52&#34;&gt;Readings  &lt;/h1&gt;

&lt;p&gt;
http://www.tcs.tifr.res.in/~prahladh/teaching/2009-10/limits/lectures/lec05.pdf

&lt;center&gt;&lt;h1 id=&#34;___sec53&#34;&gt;Lecture 12: Learning Theory  &lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;

&lt;h1 id=&#34;___sec54&#34;&gt;Probably Approximately Correct!  &lt;/h1&gt;

&lt;p&gt;
See &lt;a href=&#34;http://www.cs.utexas.edu/~klivans/f06lec2.pdf&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;http://www.cs.utexas.edu/~klivans/f06lec2.pdf&lt;/tt&gt;&lt;/a&gt; , Sections 9.1.2 and 9.1.3

&lt;h1 id=&#34;___sec55&#34;&gt;Learning Rectangles  &lt;/h1&gt;

&lt;p&gt;
See &lt;a href=&#34;http://www.cs.utexas.edu/~klivans/f06lec2.pdf&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;http://www.cs.utexas.edu/~klivans/f06lec2.pdf&lt;/tt&gt;&lt;/a&gt;  , Section 9.2.1

&lt;h1 id=&#34;___sec56&#34;&gt;Learning CNFs  &lt;/h1&gt;

&lt;p&gt;
See &lt;a href=&#34;http://www.cs.utexas.edu/~klivans/f06lec2.pdf&#34; target=&#34;_self&#34;&gt;&lt;tt&gt;http://www.cs.utexas.edu/~klivans/f06lec2.pdf&lt;/tt&gt;&lt;/a&gt;  , Section 9.2.2

&lt;h1 id=&#34;___sec57&#34;&gt;Readings  &lt;/h1&gt;

&lt;p&gt;

&lt;!-- ------------------- end of main content --------------- --&gt;


&lt;center style=&#34;font-size:80%&#34;&gt;
&lt;!-- copyright --&gt; &amp;copy; 2017-2017, Girish Varma EMAIL: girish.varma@iiit.ac.in
&lt;/center&gt;


&lt;/body&gt;
&lt;/html&gt;
    

</description>
    </item>
    
  </channel>
</rss>
